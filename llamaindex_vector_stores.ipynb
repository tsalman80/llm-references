{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "50090a1a905847e4a32e1ee94b387005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_024e8240fff246c6af50b372b175d481",
              "IPY_MODEL_7ca34bb8a6174036b8150363a68ba733",
              "IPY_MODEL_aaf7609e5f3044fcab76e90fad6e1dbe"
            ],
            "layout": "IPY_MODEL_6f7fac2c8b764e46a25393d29843d807"
          }
        },
        "024e8240fff246c6af50b372b175d481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c02e809e3725456b85d46cbddee0956c",
            "placeholder": "​",
            "style": "IPY_MODEL_ffac10754aa945ffa983c5e89eb45337",
            "value": "Parsing nodes: 100%"
          }
        },
        "7ca34bb8a6174036b8150363a68ba733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb823eb791964f2ca72274df6c16d909",
            "max": 19,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cad42c49057447efbd09d5635048d65c",
            "value": 19
          }
        },
        "aaf7609e5f3044fcab76e90fad6e1dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d323d3db33e6400b97604169d9bd3702",
            "placeholder": "​",
            "style": "IPY_MODEL_43a392b2d250478db39758426e57ba7e",
            "value": " 19/19 [00:00&lt;00:00, 145.56it/s]"
          }
        },
        "6f7fac2c8b764e46a25393d29843d807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c02e809e3725456b85d46cbddee0956c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffac10754aa945ffa983c5e89eb45337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb823eb791964f2ca72274df6c16d909": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cad42c49057447efbd09d5635048d65c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d323d3db33e6400b97604169d9bd3702": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a392b2d250478db39758426e57ba7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6487c86402404a55b65c0c2300d23e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f0cc711125d47639ccb5097275d9ddb",
              "IPY_MODEL_90b0e9f87f844241a2bd71ba70649d29",
              "IPY_MODEL_91c9451d22e14c0d8c850cbce10ca4f8"
            ],
            "layout": "IPY_MODEL_2048854a89c54fb7a4ffff3c2088c93c"
          }
        },
        "4f0cc711125d47639ccb5097275d9ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb47f2adc1e3408ea3ea20838110fe7d",
            "placeholder": "​",
            "style": "IPY_MODEL_530ba97fa1fc4caf9c28a0edeb81fbd6",
            "value": "modules.json: 100%"
          }
        },
        "90b0e9f87f844241a2bd71ba70649d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbecb7a0f9254d7f950d87e0fb9f7213",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32dce4f77ede4c3a8bf6d7c48edac0ad",
            "value": 349
          }
        },
        "91c9451d22e14c0d8c850cbce10ca4f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb032fab0d8c41809fa277a2d1459ac2",
            "placeholder": "​",
            "style": "IPY_MODEL_fa123fd9fba34c279c7a612b3cfad602",
            "value": " 349/349 [00:00&lt;00:00, 38.4kB/s]"
          }
        },
        "2048854a89c54fb7a4ffff3c2088c93c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb47f2adc1e3408ea3ea20838110fe7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "530ba97fa1fc4caf9c28a0edeb81fbd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbecb7a0f9254d7f950d87e0fb9f7213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32dce4f77ede4c3a8bf6d7c48edac0ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb032fab0d8c41809fa277a2d1459ac2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa123fd9fba34c279c7a612b3cfad602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "585f8c0f87fe47d3a6eb3c750d6567ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0effdaee342b4871abb50088f14c6a31",
              "IPY_MODEL_8c1feecd99774a0eb65e9f009fc8e907",
              "IPY_MODEL_097979c786f3403280c673e880d7040a"
            ],
            "layout": "IPY_MODEL_585dca54f17f43468b2899f8bf53cbd7"
          }
        },
        "0effdaee342b4871abb50088f14c6a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd769a014cd647348fd998fd01de1456",
            "placeholder": "​",
            "style": "IPY_MODEL_edb9877a50434060864202fcf91862e4",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "8c1feecd99774a0eb65e9f009fc8e907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf84a30624be445bb594d08415964411",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87a8a98697c74d8babe16deb6861e8be",
            "value": 124
          }
        },
        "097979c786f3403280c673e880d7040a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63a251739ae44a94b43ca7120f9ca506",
            "placeholder": "​",
            "style": "IPY_MODEL_3def064ddadf4d96834426d328775139",
            "value": " 124/124 [00:00&lt;00:00, 14.2kB/s]"
          }
        },
        "585dca54f17f43468b2899f8bf53cbd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd769a014cd647348fd998fd01de1456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edb9877a50434060864202fcf91862e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf84a30624be445bb594d08415964411": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87a8a98697c74d8babe16deb6861e8be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63a251739ae44a94b43ca7120f9ca506": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3def064ddadf4d96834426d328775139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54c144fda9b44182ae52fff76c703f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7919b7d33e8346269fa9ca36b4027ed4",
              "IPY_MODEL_f99f756949984059af5a56867073cdef",
              "IPY_MODEL_dcfc01aba66c46cb9aa7a805485c2676"
            ],
            "layout": "IPY_MODEL_a3556f7158084884a6c59b33bdf750b7"
          }
        },
        "7919b7d33e8346269fa9ca36b4027ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87ad42306bf94ccc89e4ad052a3d6d57",
            "placeholder": "​",
            "style": "IPY_MODEL_f14b4da1bdc34b87be985abf78530f7a",
            "value": "README.md: 100%"
          }
        },
        "f99f756949984059af5a56867073cdef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8b05376e7f94bffaa313dd526fef46d",
            "max": 94783,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e6c754afe4c4b658b114ac04b659d06",
            "value": 94783
          }
        },
        "dcfc01aba66c46cb9aa7a805485c2676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a72fd6ce2d924d12911a812d8f845eef",
            "placeholder": "​",
            "style": "IPY_MODEL_3d260ecd240544aa94fd90eded12c660",
            "value": " 94.8k/94.8k [00:00&lt;00:00, 2.98MB/s]"
          }
        },
        "a3556f7158084884a6c59b33bdf750b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87ad42306bf94ccc89e4ad052a3d6d57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f14b4da1bdc34b87be985abf78530f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8b05376e7f94bffaa313dd526fef46d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e6c754afe4c4b658b114ac04b659d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a72fd6ce2d924d12911a812d8f845eef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d260ecd240544aa94fd90eded12c660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52907b61e1b041db88a74c8a39d88ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42827a5c847c4f60a002b27bcb0c82a9",
              "IPY_MODEL_ee37016a0afa4dafb58563c39a5c5dc2",
              "IPY_MODEL_78812a7bfdfb431489f9f73389932cb6"
            ],
            "layout": "IPY_MODEL_d4895c0aa7e34d92bc73e640411b8359"
          }
        },
        "42827a5c847c4f60a002b27bcb0c82a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0b578c213df467b981572be91af1eac",
            "placeholder": "​",
            "style": "IPY_MODEL_525e7fed7c7341fe92527610b10e6257",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "ee37016a0afa4dafb58563c39a5c5dc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c14adf4d49848bc8ac702fc6c86e9a5",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_053e521768ee4c6cbbc7df1b7d6c35d3",
            "value": 52
          }
        },
        "78812a7bfdfb431489f9f73389932cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2d1b57fa636448e9b066e3341ba4958",
            "placeholder": "​",
            "style": "IPY_MODEL_d528bb4242784a019d66ccd22744e7e1",
            "value": " 52.0/52.0 [00:00&lt;00:00, 6.19kB/s]"
          }
        },
        "d4895c0aa7e34d92bc73e640411b8359": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b578c213df467b981572be91af1eac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "525e7fed7c7341fe92527610b10e6257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c14adf4d49848bc8ac702fc6c86e9a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "053e521768ee4c6cbbc7df1b7d6c35d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2d1b57fa636448e9b066e3341ba4958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d528bb4242784a019d66ccd22744e7e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0b89fa2b3f040e481493959808f63a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9daaa444e93c4dda911a79bf6158e401",
              "IPY_MODEL_102bfa341d0041a09aace6a9a1ad6927",
              "IPY_MODEL_94af3670682749a5955fb63eab6b0a80"
            ],
            "layout": "IPY_MODEL_19e3717c839941129c847079fb85587a"
          }
        },
        "9daaa444e93c4dda911a79bf6158e401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f333896e5f7e404b8d2090f2d1896857",
            "placeholder": "​",
            "style": "IPY_MODEL_1c6ea5761541414ba2648913c9f68591",
            "value": "config.json: 100%"
          }
        },
        "102bfa341d0041a09aace6a9a1ad6927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04f40645b35740449376e1f5cde65655",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38b1bc8a8fa445ff8d38aeff74007294",
            "value": 743
          }
        },
        "94af3670682749a5955fb63eab6b0a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0540b71ea39d41278b3e19e2b2479f2a",
            "placeholder": "​",
            "style": "IPY_MODEL_d2bd5c77415e40f39673a8d713916b25",
            "value": " 743/743 [00:00&lt;00:00, 79.9kB/s]"
          }
        },
        "19e3717c839941129c847079fb85587a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f333896e5f7e404b8d2090f2d1896857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c6ea5761541414ba2648913c9f68591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04f40645b35740449376e1f5cde65655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38b1bc8a8fa445ff8d38aeff74007294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0540b71ea39d41278b3e19e2b2479f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2bd5c77415e40f39673a8d713916b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc3292b9c7d74e8588e3dfb0105d2695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0e1958aa62448a99d464622743c381c",
              "IPY_MODEL_aa9ef684b2b444b9be45eeeacff70e8f",
              "IPY_MODEL_78fb05bf5a0b4ca3aaaa5b06da0659b7"
            ],
            "layout": "IPY_MODEL_2643a589bed14e0998b7dabef4f5ed94"
          }
        },
        "c0e1958aa62448a99d464622743c381c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b09473fa8eee42338ca04d37215be237",
            "placeholder": "​",
            "style": "IPY_MODEL_f1e5880d8e204ea0b24bcabcb12fca64",
            "value": "model.safetensors: 100%"
          }
        },
        "aa9ef684b2b444b9be45eeeacff70e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f032fdb793f46749dce7190fb32afe9",
            "max": 133466304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f56f29eaa33d4270b88a40d8fae5262d",
            "value": 133466304
          }
        },
        "78fb05bf5a0b4ca3aaaa5b06da0659b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_862455d1cd7f486b8e5d63471022cd5b",
            "placeholder": "​",
            "style": "IPY_MODEL_1e3d0cca925b4e0f9bc7cc6e267713dd",
            "value": " 133M/133M [00:01&lt;00:00, 91.5MB/s]"
          }
        },
        "2643a589bed14e0998b7dabef4f5ed94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b09473fa8eee42338ca04d37215be237": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e5880d8e204ea0b24bcabcb12fca64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f032fdb793f46749dce7190fb32afe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f56f29eaa33d4270b88a40d8fae5262d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "862455d1cd7f486b8e5d63471022cd5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e3d0cca925b4e0f9bc7cc6e267713dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af14f159fee14c63886c824e9e13d9de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b66ab1a9d5424b02a333062249287183",
              "IPY_MODEL_c3e41cd749564462add3634d87b80101",
              "IPY_MODEL_e26f6f229c0c4bb6be571a989b5c925a"
            ],
            "layout": "IPY_MODEL_33869482cc494ba6ad61ab8acef98fa3"
          }
        },
        "b66ab1a9d5424b02a333062249287183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b4f2b94cf7a449f87d7e89ef022ea76",
            "placeholder": "​",
            "style": "IPY_MODEL_ed2b6611e381425eaf66ee91435c5b4a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c3e41cd749564462add3634d87b80101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ab82be85e084df89f835bd6c878c496",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9caae9bdc41400982d4d9db05990d7c",
            "value": 366
          }
        },
        "e26f6f229c0c4bb6be571a989b5c925a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02c9bc1b82254197b13c480a8b501b1c",
            "placeholder": "​",
            "style": "IPY_MODEL_94d6dfcc9c6b4837ac53fc26c8ba3501",
            "value": " 366/366 [00:00&lt;00:00, 31.3kB/s]"
          }
        },
        "33869482cc494ba6ad61ab8acef98fa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b4f2b94cf7a449f87d7e89ef022ea76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed2b6611e381425eaf66ee91435c5b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ab82be85e084df89f835bd6c878c496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9caae9bdc41400982d4d9db05990d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02c9bc1b82254197b13c480a8b501b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94d6dfcc9c6b4837ac53fc26c8ba3501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bac2207e58b451886b4392d2df2d605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fda8960bbcb74542b42f9f6a5cdce908",
              "IPY_MODEL_d46ae126e36847acb2a5c59c9e63ac2b",
              "IPY_MODEL_8315ec95bf0143a6a6d9c2d7f1c752c4"
            ],
            "layout": "IPY_MODEL_3ed4884f50594ac5b218b3def9154996"
          }
        },
        "fda8960bbcb74542b42f9f6a5cdce908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_331d0b5e1964461aa7cf518d53324230",
            "placeholder": "​",
            "style": "IPY_MODEL_083b917f31364466a1ad859cd42b098a",
            "value": "vocab.txt: 100%"
          }
        },
        "d46ae126e36847acb2a5c59c9e63ac2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a2c13d0b9fd4425bce6bbc7c589f221",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f072c6685f8046fc8df3fea11b471b75",
            "value": 231508
          }
        },
        "8315ec95bf0143a6a6d9c2d7f1c752c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_684a7baac1904888aac73f5c627b1193",
            "placeholder": "​",
            "style": "IPY_MODEL_a48a98e17ce64b2681b542b1740de270",
            "value": " 232k/232k [00:00&lt;00:00, 10.7MB/s]"
          }
        },
        "3ed4884f50594ac5b218b3def9154996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331d0b5e1964461aa7cf518d53324230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "083b917f31364466a1ad859cd42b098a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a2c13d0b9fd4425bce6bbc7c589f221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f072c6685f8046fc8df3fea11b471b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "684a7baac1904888aac73f5c627b1193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a48a98e17ce64b2681b542b1740de270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3d69e5a99114c618b6a124e41c1ad0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3483b19492e447b9b64dbc7a07b5c58d",
              "IPY_MODEL_5ca994e4684d4e32b0548f3d402e0679",
              "IPY_MODEL_d21de1bb5a584b1d83aeac4af6b3d6a4"
            ],
            "layout": "IPY_MODEL_0cca6ca326434305919d1717799f4e7a"
          }
        },
        "3483b19492e447b9b64dbc7a07b5c58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2634d4a8ec649b6af98f0095a69865f",
            "placeholder": "​",
            "style": "IPY_MODEL_77ad6a3030694bd79778c286826e77fa",
            "value": "tokenizer.json: 100%"
          }
        },
        "5ca994e4684d4e32b0548f3d402e0679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece3bf82fc4f4d18abecd60268808c56",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3bb7903cd344bcab50747840e4e3043",
            "value": 711396
          }
        },
        "d21de1bb5a584b1d83aeac4af6b3d6a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5282e8fd1a414f8eb588dc87163d716e",
            "placeholder": "​",
            "style": "IPY_MODEL_6cf4367a56754124b4963d51f8cb4159",
            "value": " 711k/711k [00:00&lt;00:00, 24.9MB/s]"
          }
        },
        "0cca6ca326434305919d1717799f4e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2634d4a8ec649b6af98f0095a69865f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77ad6a3030694bd79778c286826e77fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ece3bf82fc4f4d18abecd60268808c56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3bb7903cd344bcab50747840e4e3043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5282e8fd1a414f8eb588dc87163d716e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cf4367a56754124b4963d51f8cb4159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de522a701f104625a2d7f55cf40c473e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3c8d1aa7fa94e9ebd9b45167bce063a",
              "IPY_MODEL_84d26ce841754159a704ac2c2c5e5160",
              "IPY_MODEL_6fb2fd40f99d4f8ea02107a6216de139"
            ],
            "layout": "IPY_MODEL_3776261b4cdb4ffbbf68d404f8bc0ff8"
          }
        },
        "f3c8d1aa7fa94e9ebd9b45167bce063a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c5d1e19a8a14bc4bbb833ecc558c6f7",
            "placeholder": "​",
            "style": "IPY_MODEL_300cec9a848b4c29bc5320db8bd433d4",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "84d26ce841754159a704ac2c2c5e5160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_699655708fb74eef96c376661a1e3353",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be20848f4266409e9c52fe4df00c1cfc",
            "value": 125
          }
        },
        "6fb2fd40f99d4f8ea02107a6216de139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e6215f9695a4f3c8a0b2ec55b5d5e14",
            "placeholder": "​",
            "style": "IPY_MODEL_01beea19411141238a63870b26f5879c",
            "value": " 125/125 [00:00&lt;00:00, 11.3kB/s]"
          }
        },
        "3776261b4cdb4ffbbf68d404f8bc0ff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c5d1e19a8a14bc4bbb833ecc558c6f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "300cec9a848b4c29bc5320db8bd433d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "699655708fb74eef96c376661a1e3353": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be20848f4266409e9c52fe4df00c1cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e6215f9695a4f3c8a0b2ec55b5d5e14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01beea19411141238a63870b26f5879c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0822f2758e44404186c0fe0f65f5b7b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55c2515d42424875a808f20acf372620",
              "IPY_MODEL_c2106309ac66405f9ba758730451477f",
              "IPY_MODEL_dd70d845be0b4f3888513cec6d48a390"
            ],
            "layout": "IPY_MODEL_12154920aa1f4358a54cc93ef8d76d6a"
          }
        },
        "55c2515d42424875a808f20acf372620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbe1643c6ab047b5b3ff882a19890553",
            "placeholder": "​",
            "style": "IPY_MODEL_e6edcb2f263741bba36c63b91c281d72",
            "value": "1_Pooling%2Fconfig.json: 100%"
          }
        },
        "c2106309ac66405f9ba758730451477f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75c3acc0c14642e88d04eb5ff5014954",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45d4a79a58554a62a64bb2d8df9dc888",
            "value": 190
          }
        },
        "dd70d845be0b4f3888513cec6d48a390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2500cdff07fe40d9a1e47fd7081d4297",
            "placeholder": "​",
            "style": "IPY_MODEL_a31a75ba72e94b4895d64ce53ad4e6b2",
            "value": " 190/190 [00:00&lt;00:00, 12.8kB/s]"
          }
        },
        "12154920aa1f4358a54cc93ef8d76d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbe1643c6ab047b5b3ff882a19890553": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6edcb2f263741bba36c63b91c281d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75c3acc0c14642e88d04eb5ff5014954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45d4a79a58554a62a64bb2d8df9dc888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2500cdff07fe40d9a1e47fd7081d4297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a31a75ba72e94b4895d64ce53ad4e6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Advanced RAG in LlamaIndex"
      ],
      "metadata": {
        "id": "b9MrU1VhgZq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqG2gONM1TIn",
        "outputId": "cd6ef64d-8fcc-4c11-f435-bed43a4760d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "bZnZasH81VBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aNITVh6fcZRH"
      },
      "outputs": [],
      "source": [
        "%pip install -Uq llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract"
      ],
      "metadata": {
        "id": "OtblxeGJgfqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "docs = SimpleDirectoryReader(input_dir=\"./data\").load_data()\n",
        "\n",
        "# file name as id\n",
        "# docs_nam_as_id = SimpleDirectoryReader(input_dir=\"./data\", filename_as_id=True).load_data()"
      ],
      "metadata": {
        "id": "ew6PmzeJcgZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)  # one per page"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6_Qs8oFd5U_",
        "outputId": "b1d415e3-f362-488b-896b-c916f5e9d64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pprint.pprint(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Kr3oxI_1burS",
        "outputId": "5e1fd25a-96c6-469e-85d7-0d96737fd24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id_='6e6bc31c-24ec-46d9-a214-8010c98d526d', embedding=None, metadata={'page_label': '1', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='HealthGPT: A Medical Large Vision-Language Model for Unifying\\nComprehension and Generation via Heterogeneous Knowledge Adaptation\\nTianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\\nMengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\\n1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\\n4The Hong Kong University of Science and Technology,5National University of Singapore\\nProject Page\\n Code\\n1. X-Ray \\nComprehension\\n2. CT\\nComprehension\\n3. MRI\\nComprehension\\nComp. Perf.\\n7 Medical Multi-Modal Comprehension Tasks\\nGen. \\nPerformance\\n5 Medical Multi-Modal Generation Tasks\\nList all anatomical locations showing \\npulmonary edema, hazy opacity, or \\nmediastinal displacement.\\nLeft hilar structures, left lung,\\nright hilar structures, right lung.\\nWhich abdominal organ shows any \\nindication of a lesion or abnormality \\nin the CT image?\\nNo abdominal organs show\\nany clear indications of lesions\\nor abnormalities.\\nCould you explain what this mass\\nin the MRI means for my health? \\nIs it very serious?\\nCertainly, the MRI shows a\\ndefined mass in your left nasal\\ncavity. There is no sign of the ...\\n4. Microsopy Comprehension\\n What is the purpose of the different \\nmembrane treatments used in this \\nstudy?\\nThe purpose of the different\\nmembrane treatments used…\\n5. OCT Comprehension\\n What is the purpose of comparing \\nthe OCT structure image and OCTA \\nimage with H&E histology?\\nTo confirm the histological position\\nof the obtained OCT brain images.\\n6. Fundus\\nComprehension\\n What specific findings or pathological \\nchanges can be observed in this \\nfundus image?\\nThe fundus image appears normal with\\nno noticeable signs of pathology…\\n7. Ultrasound\\nComprehension\\n What type of imaging technique \\nis used in this image?\\nThe image is a sagittal gray-\\nscale ultrasonographic…\\n1. CT2MRI\\nGeneration\\nI need a version of this CT representation \\nin MRI.\\nThe image has\\nbeen transformed\\ninto MRI.\\n2. MRI2CT\\nGeneration\\nTransform the MRI display into a \\nCT image.\\nHere is the CT\\nversion of the\\nMRI image.\\n3. Image Reconstruction\\nReconstruct the following \\nmedical images.\\nHere is the reconstructed\\nmedical image you need.\\n4. Super Resolution\\nCould you improve the quality\\nof this MRI image?\\nHere is the image with\\nimproved resolution.\\n5. Report-to-CXR\\nThe X-ray shows no \\npleural effusion or \\npneumothorax.\\nHere is the\\nchest X-ray\\nimage for\\nyou.\\nGen. Perf.\\nFigure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\\nunified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\\nplex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\\nAbstract\\nWe present HealthGPT, a powerful Medical Large Vision-\\nLanguage Model (Med-LVLM) that integrates medical vi-\\nsual comprehension and generation capabilities within a uni-\\nfied autoregressive paradigm. Our bootstrapping philosophy\\nis to progressively adapt heterogeneous comprehension and\\ngeneration knowledge to pre-trained large language mod-\\nels (LLMs). This is achieved through a novel heterogeneous\\nlow-rank adaptation (H-LoRA) technique, which is com-\\nplemented by a tailored hierarchical visual perception ap-\\nproach and a three-stage learning strategy. To effectively\\nlearn the HealthGPT, we devise a comprehensive medi-\\ncal domain-specific comprehension and generation dataset\\ncalled VL-Health. Experimental results demonstrate ex-\\nceptional performance and scalability of HealthGPT in\\nmedical visual unified tasks. Our project can be accessed at\\nhttps://github.com/DCDmllm/HealthGPT.\\n1 Introduction\\nLarge Vision-Language Models (LVLMs) (Liu et al. 2023;\\nOpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\\ndemonstrated outstanding open-world visual comprehension\\nand reasoning abilities through language-based interactive\\ndialogue over the past years, simultaneously opening up\\nnew opportunities for applications in specialized domains.\\n1\\narXiv:2502.09838v2  [cs.CV]  17 Feb 2025', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='7f452a19-240e-4355-8d24-7091020b52ed', embedding=None, metadata={'page_label': '2', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Specifically, recent studies (Li et al. 2024a; Tu et al. 2024)\\nhave utilized pre-trained large language models (LLMs) and\\nvisual instruction data to build interactive diagnostic tools\\nand treatment planning systems, revealing the immense po-\\ntential of LVLMs in medical scenarios. However, these stud-\\nies primarily concentrate on visual comprehension tasks that\\nproduce text-based outputs, such as medical visual ques-\\ntion answering (Li et al. 2024a) or report generation (Nath\\net al. 2024), and deficient the “drawing” capability needed\\nfor medical visual generation. In practice, integrating visual\\ncomprehension and generation can significantly enhance the\\nmultifunctionality of medical LVLMs.\\nRecent studies have increasingly focused on developing\\nunified LVLMs capable of comprehending and generating\\ncontent across diverse visual modalities. Earlier approaches\\npredominantly utilized continuous visual tokens fed into\\nLLMs, using the LLMs themselves as conditional genera-\\ntors for external generative models (Ge et al. 2024; Wu et al.\\n2023; Dong et al. 2023). More recent research has explored\\nthe use of discrete visual tokens for image representation and\\ngeneration within a fully autoregressive framework (Team\\n2024; Wang et al. 2024a; Xie et al. 2024). These meth-\\nods not only enhance controllability but also demonstrate\\nearly success in open-world, any-to-any tasks, highlighting\\nthe preliminary potential of a unified autoregressive learning\\nparadigm in multi-modal tasks.\\nWhile unified LVLMs have achieved initial success in\\ngeneral scenarios, such a unified framework remains under-\\nexplored in the medical domain. Adapting the aforemen-\\ntioned general unified model paradigm to the medical do-\\nmain presents two major challenges: (i) High-scale and\\n-quality Data Limitations . Open-world models necessi-\\ntate extensive pre-training on billions or even more diverse,\\nmulti-modal data samples for comprehension and genera-\\ntion tasks (Lu et al. 2024; Team 2024). However, the ac-\\ncessible medical data significantly lacks in scale and qual-\\nity compared to natural multi-modal datasets. Its special-\\nized and domain-specific characteristics make it challenging\\nto develop a unified medical model from scratch. (ii) Con-\\nflicts between Comprehension and Generation . Compre-\\nhension tasks often strip away visual details to focus on\\nabstraction, while generation tasks require detailed preser-\\nvation, making tokens sensitive to all visual alterations. As\\nshown in Figure 2, which features experiments conducted on\\nmedical images, the performance in comprehension (or gen-\\neration) tasks steadily decreases as the proportion of genera-\\ntion (or comprehension) data increases, and vice versa. This\\nhighlights a dilemma in autoregressive multi-modal training,\\nstemming from the need to maintain consistency between\\npre- and post-LVLMs. While some methods have explored\\nmutual enhancement between comprehension and genera-\\ntion (Pan et al. 2024; Tong et al. 2024), improvements still\\nexhibit diminishing returns, with performance degradation\\nremaining a significant issue.\\n(a) (b)\\nFigure 2: With a fixed amount of comprehension (genera-\\ntion) data, increasing the proportion of the other type leads\\nto significant performance degradation.\\nTo tackle the aforementioned challenges, we propose\\nHealthGPT (see Figure 1) , which progressively adapts a\\npre-trained LLM as an unified medical multi-modal model\\nwith a small amount of visual instruction data. We de-\\nvise innovative Parameter-Efficient Fine-Tuning (PEFT) ap-\\nproach (Ding et al. 2023), calledHeterogeneous Low-Rank\\nAdaptation (H-LoRA), which decouples the learning pro-\\ncess of LVLMs for comprehension and generation tasks. In-\\nspired by the plug-and-play nature of LoRA (Hu et al. 2021),\\nH-LoRA enables the model to store heterogeneous compre-\\nhension and generation knowledge in independent “plug-\\nins”, thus avoiding joint optimization issues caused by con-\\nflicts between comprehension and generation tasks. In addi-\\ntion, we also consider the variety of sub-tasks among com-\\nprehension or generation tasks. Qualitative research high-\\nlights the limitations of a single LoRA in handling multi-\\ndimensional task scenarios, mainly due to catastrophic for-\\ngetting and interference (Liu et al. 2024d; Lin et al. 2024).\\nTo address this, we draw on the concept of Mixture of Ex-\\nperts (MoE) (Masoudnia and Ebrahimpour 2014) and in-\\ntroduce LoRA experts. The aim is to dynamically transfer\\ntask-shared knowledge to adapt to downstream tasks. Unlike\\nMoELoRA (Luo et al. 2024a), H-LoRA employs reversible\\nmatrix block multiplication to combine LoRA experts, sig-\\nnificantly reducing the overhead of multiple matrix multi-\\nplications. Notably, when using four experts, it requires\\nonly 67% of the MoELoRA training time.\\nTo effectively leverage H-LoRA inHealthGPT, we fur-\\nther introduce a Hierarchical Visual Perception (HVP)\\nand devise a correspondingThree-stage Learning Strategy\\n(TLS). HVP: we separate visual details learning from Vi-\\nsion transformer (ViT) for comprehension and generation.\\nAs is widely recognized, the ViT encodes visual concepts\\nwith increasing abstraction, generally, becoming finer as we\\nprogress over levels (Vig 2019). Thus, we maintain the vi-\\nsual features of the anterior and posterior layers to accom-\\nmodate the differing requirements for visual granularity in\\ncomprehension and generation tasks while preventing po-\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='53eaa224-d052-4454-8191-399e7f14634e', embedding=None, metadata={'page_label': '3', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='tential task interference. TLS: In the first and second stages,\\ngiven the heterogeneity between comprehension and gener-\\nation tasks, we first train H-LoRA plugins for HealthGPT\\nto incorporate both medical comprehension and generation\\nknowledge, thus endowing the LLMs with capabilities for\\nvision-language alignment and vision-to-vision reconstruc-\\ntion. Additionally, through minimal mixed-task training, we\\nbuilt fusion embedding layers and output heads that merge\\ntext and visual tokens, establishing a unified LVLM founda-\\ntion for visual instruction fine-tuning. In the third stage, by\\nonly training the H-LoRA plugins, HealthGPT is able to\\nrapidly adapt to a wide range of downstream medical tasks,\\ncovering various types of medical comprehension and gen-\\neration tasks.\\nTo effectively implement our approach, we have cu-\\nrated a dataset for training unified medical LVLMs, called\\nVL-Health, including seven comprehension tasks and five\\ngeneration tasks (Figure 1). Through quantitative analysis\\nand validation on multi-modal tasks, the results demonstrate\\nthat HealthGPT is capable of unifying medical multi-\\nmodal abilities in data-constrained scenarios, achieving per-\\nformance comparable to or better than existing state-of-the-\\nart (SOTA) models across multiple metrics. Overall, the\\nmain contributions of this paper are summarized as follows:\\n• Unified Med-LVLM. We introduce HealthGPT,\\nwhich, to the best of our knowledge, is the first unified\\nframework for multi-modal comprehension and genera-\\ntion in complex medical scenarios.\\n• Effective Learning Paradigm. We present H-LoRA, an\\noptimized multi-LoRA PEFT architecture based on task-\\ngated decoupling, is designed to effectively mitigate data\\nconflict issues.\\n• Holistic Training Dataset. We curated VL-Health, a\\ncomprehensive dataset designed for both comprehension\\nand generation tasks.\\n• Superior Downstream Improvements : Extensive ex-\\nperiments are conducted and the results confirm\\nHealthGPT’s effectiveness in medical vision-language\\ncomprehension and generation.\\n2 Related Work\\nMedical Vision Large Language Models. Recently, medi-\\ncal vision large language models (Med-VLLMs) have made\\nsignificant progress, demonstrating excellent performance\\nin understanding medical images and responding to human\\nqueries based on these images (Zhou et al. 2023; Tian et al.\\n2023). XrayGPT (Thawkar et al. 2023) combines a med-\\nical visual encoder (MedClip) (Wang et al. 2022) with a\\nfine-tuned LLM , using a simple linear transformation layer\\nto achieve alignment between visual and textual informa-\\ntion, significantly enhancing the understanding of medical\\nimages. On this basis, LLaV A-Med (Li et al. 2024b) fur-\\nther enhances visual-text alignment in medical contexts by\\nselecting high-quality image-text pairs from PubMed pa-\\npers and synthesized VQA datasets. BiomedGPT (Luo et al.\\n2024b) employs a BERT-style encoder and GPT-style de-\\ncoder architecture, pre-trained on interdisciplinary datasets.\\nCompared to commercial models like Med-PaLM (Singhal\\net al. 2023), BiomedGPT significantly reduces model size\\nwhile maintaining superior performance. However, issues\\nof language adaptability and dataset specificity still remain.\\nTo address these, HuatuoGPT-Vision (Chen et al. 2024a)\\nintroduces the PubMedVision dataset, which contains 1.3\\nmillion high-quality medical samples, significantly improv-\\ning the model’s adaptability across diverse medical applica-\\ntions. However, current Med-VLLMs mainly focus on med-\\nical comprehension and lack the capability for the medical\\nvision-language generation.\\nUnified Visual Comprehension and Generation Mod-\\nels. Recent research has increasingly concentrated on cre-\\nating unified LVLMs that are adept at understanding and\\nproducing content across various visual modalities. NExT-\\nGPT (Wu et al. 2023) achieves perception and generation for\\narbitrary combinations of multi-modal inputs and outputs by\\naligning LLMs. Similarly, SEED (Ge et al. 2023), SEED-\\nX (Ge et al. 2024), and DreamLLM (Dong et al. 2023) em-\\nploy learnable queries and leverage next-token prediction to\\ngenerate visual tokens, providing conditional inputs to exter-\\nnal generation modules. Unlike these methods, which func-\\ntion as external conditioners, Unified-IO (Lu et al. 2022),\\nUnified-IO 2 (Lu et al. 2024), and Chameleon (Team 2024)\\ninternalize multi-modal generation tasks within a unified\\nTransformer architecture by extending multi-modal vocab-\\nularies, enabling direct generation based on next-token pre-\\ndiction. Building on this concept, Lumina-mGPT (Liu et al.\\n2024a) and ANOLE (Chern et al. 2024) further enhance the\\ngeneration capabilities of unified models using high-quality\\ndata, particularly improving the quality and flexibility of im-\\nage generation.\\n3 Preliminaries\\nLarge Vision-Language Models.The input to a LVLM typ-\\nically consists of an image ximg and a discrete text sequence\\nxtxt. The visual encoder Eimg converts the input image ximg\\ninto a sequence of visual tokens V = [ vi]Nv\\ni=1, while the\\ntext sequence xtxt is mapped into a sequence of text to-\\nkens T = [ ti]Nt\\ni=1 using an embedding function Etxt. The\\nLLM MLLM(·|θ) models the joint probability of the token\\nsequence U = {V, T }, which is expressed as:\\nPθ(R|U) =\\nNrY\\ni=1\\nPθ(ri|{U, r<i}), (1)\\nwhere R = [ri]Nr\\ni=1 is the text response sequence. The LVLM\\niteratively generates the next token ri based on r<i. The op-\\ntimization objective is to minimize the cross-entropy loss of\\nthe response R. It is worth noting that most LVLMs adopt\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='9620a010-a615-4ef3-b724-bc33b0bb3746', embedding=None, metadata={'page_label': '4', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 3: The HealthGPT architecture integrates hierarchical visual perception and H-LoRA, employing a task-specific hard\\nrouter to select visual features and H-LoRA plugins, ultimately generating outputs with an autoregressive manner.\\na design paradigm based on ViT, alignment adapters, and\\npre-trained LLMs(Liu et al. 2023, 2024b), enabling quick\\nadaptation to downstream tasks.\\nVQGAN. VQGAN (Esser, Rombach, and Ommer 2021)\\nemploys latent space compression and indexing mechanisms\\nto effectively learn a complete discrete representation of im-\\nages. VQGAN first maps the input imageximg to a latent rep-\\nresentation z = E(x) through a encoder E. Then, the latent\\nrepresentation is quantized using a codebookZ = {zk}K\\nk=1,\\ngenerating a discrete index sequence I = [im]N\\nm=1, where\\nim ∈ Zrepresents the quantized code index:\\nI = Quantize(z|Z) = arg min\\nzk∈Z\\n∥z − zk∥2. (2)\\nIn our approach, the discrete index sequence I serves as\\na supervisory signal for the generation task, enabling the\\nmodel to predict the index sequence ˆI from input conditions\\nsuch as text or other modality signals. Finally, the predicted\\nindex sequence ˆI is upsampled by the VQGAN decoder G,\\ngenerating the high-quality image ˆximg = G(ˆI).\\nLow Rank Adaptation. LoRA(Hu et al. 2021) effectively\\ncaptures the characteristics of downstream tasks by intro-\\nducing low-rank adapters. The core idea is to decompose\\nthe bypass weight matrix ∆W ∈ Rdin×dout\\ninto two low-\\nrank matrices {A ∈ Rdin×r, B ∈ Rr×dout\\n}, where r ≪\\nmin{din, dout}, significantly reducing learnable parameters.\\nThe output with the LoRA adapter for the input x is then\\ngiven by:\\nh = xW0 + αx∆W/r = xW0 + αxAB/r, (3)\\nwhere matrix A is initialized with a Gaussian distribution,\\nwhile the matrixB is initialized as a zero matrix. The scaling\\nfactor α/r controls the impact of ∆W on the model.\\n4 HealthGPT\\n4.1 Unified Autoregressive Generation.\\nHealthGPT (Figure 3) utilizes a discrete token representa-\\ntion that covers both text and visual outputs, unifying visual\\ncomprehension and generation as an autoregressive task. For\\ncomprehension, Mllm receives the input joint sequence U\\nand outputs a series of text token R = [ r1, r2, . . . , rNr ],\\nwhere ri ∈ Vtxt, and Vtxt represents the LLM’s vocabulary:\\nPθ(R | U) =\\nNrY\\ni=1\\nPθ(ri | U, r<i). (4)\\nFor generation, Mllm first receives a special start token\\n⟨START IMG⟩, then generates a series of tokens corre-\\nsponding to the VQGAN indices I = [ i1, i2, . . . , iNi ],\\nwhere ij ∈ Vvq, and Vvq represents the index range of VQ-\\nGAN. Upon completion of generation, the LLM outputs an\\nend token ⟨END IMG⟩:\\nPθ(I | U) =\\nNiY\\nj=1\\nPθ(ij | U, i<j). (5)\\nFinally, the generated index sequence I is fed into the de-\\ncoder G, which reconstructs the target image ˆximg = G(I).\\n4.2 Hierarchical Visual Perception\\nGiven the differences in visual perception between compre-\\nhension and generation tasks—where the former focuses on\\nabstract semantics and the latter emphasizes complete se-\\nmantics—we employ ViT to compress the image into dis-\\ncrete visual tokens at multiple hierarchical levels. Specif-\\nically, the image is converted into a series of features\\n{f1, f2, . . . , fL} as it passes through L ViT blocks.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='bb8384b0-18f1-449e-8e04-3d643f33a184', embedding=None, metadata={'page_label': '5', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='To address the needs of various tasks, the hidden states\\nare divided into two types: (i) Concrete-grained features\\nFCon = {f1, f2, . . . , fk}, k < L, derived from the shal-\\nlower layers of ViT, containing sufficient global features,\\nsuitable for generation tasks; (ii) Abstract-grained features\\nFAbs = {fk+1, fk+2, . . . , fL}, derived from the deeper\\nlayers of ViT, which contain abstract semantic information\\ncloser to the text space, suitable for comprehension tasks.\\nThe task type T (comprehension or generation) deter-\\nmines which set of features is selected as the input for the\\ndownstream large language model:\\nFimg\\nT =\\n(\\nFCon, if T = generation task\\nFAbs, if T = comprehension task (6)\\nWe integrate the image featuresFimg\\nT and text featuresT into\\na joint sequence through simple concatenation, which is then\\nfed into the LLM Mllm for autoregressive generation.\\n4.3 Heterogeneous Knowledge Adaptation\\nWe devise H-LoRA, which stores heterogeneous knowledge\\nfrom comprehension and generation tasks in separate mod-\\nules and dynamically routes to extract task-relevant knowl-\\nedge from these modules. At the task level, for each task type\\nT, we dynamically assign a dedicated H-LoRA submodule\\nθT , which is expressed as:\\nR = MLLM(U|θ, θT ), θ T = {AT , BT , RT\\nouter}. (7)\\nAt the feature level for a single task, H-LoRA integrates the\\nidea of Mixture of Experts (MoE) (Masoudnia and Ebrahim-\\npour 2014) and designs an efficient matrix merging and rout-\\ning weight allocation mechanism, thus avoiding the signif-\\nicant computational delay introduced by matrix splitting in\\nexisting MoELoRA (Luo et al. 2024a). Specifically, we first\\nmerge the low-rank matrices (rank = r) of k LoRA experts\\ninto a unified matrix:\\nAmerged, Bmerged = Concat({Ai}k\\n1 ), Concat({Bi}k\\n1 ), (8)\\nwhere Amerged ∈ Rdin×rk and Bmerged ∈ Rrk×dout\\n. The\\nk-dimension routing layer generates expert weights W ∈\\nRtoken num×k based on the input hidden state x, and these are\\nexpanded to Rtoken num×rk as follows:\\nWexpanded = αkW/r ⊗ 1r, (9)\\nwhere ⊗ denotes the replication operation. The overall out-\\nput of H-LoRA is computed as:\\nOH-LoRA = (xAmerged ⊙ Wexpanded)Bmerged, (10)\\nwhere ⊙ represents element-wise multiplication. Finally, the\\noutput of H-LoRA is added to the frozen pre-trained weights\\nto produce the final output:\\nO = xW0 + OH-LoRA. (11)\\n900\\n800\\n700\\n600\\n500\\n400\\n300\\n200\\n100\\n0\\nComp. Gen.\\n(a) (b)\\n783K765K\\n（K）\\nFigure 4: Data statistics of VL-Health.\\n4.4 Training Pipeline\\n1st Stage: Multi-modal Alignment. In the first stage, we\\ndesign separate visual adapters and H-LoRA submodules for\\nmedical unified tasks. For the medical comprehension task,\\nwe train abstract-grained visual adapters using high-quality\\nimage-text pairs to align visual embeddings with textual\\nembeddings, thereby enabling the model to accurately de-\\nscribe medical visual content. During this process, the pre-\\ntrained LLM and its corresponding H-LoRA submodules\\nremain frozen. In contrast, the medical generation task re-\\nquires training concrete-grained adapters and H-LoRA sub-\\nmodules while keeping the LLM frozen. Meanwhile, we ex-\\ntend the textual vocabulary to include multimodal tokens,\\nenabling the support of additional VQGAN vector quanti-\\nzation indices. The model trains on image-VQ pairs, en-\\ndowing the pre-trained LLM with the capability for image\\nreconstruction. This design ensures pixel-level consistency\\nof pre- and post-LVLM. The processes establish the initial\\nalignment between the LLM’s outputs and the visual inputs.\\n2nd Stage: Heterogeneous H-LoRA Plugin Adaptation.\\nThe submodules of H-LoRA share the word embedding\\nlayer and output head but may encounter issues such as\\nbias and scale inconsistencies during training across dif-\\nferent tasks. To ensure that the multiple H-LoRA plugins\\nseamlessly interface with the LLMs and form a unified base,\\nwe fine-tune the word embedding layer and output head us-\\ning a small amount of mixed data to maintain consistency\\nin the model weights. Specifically, during this stage, all H-\\nLoRA submodules for different tasks are kept frozen, with\\nonly the word embedding layer and output head being op-\\ntimized. Through this stage, the model accumulates foun-\\ndational knowledge for unified tasks by adapting H-LoRA\\nplugins.\\n3rd Stage: Visual Instruction Fine-Tuning. In the third\\nstage, we introduce additional task-specific data to fur-\\nther optimize the model and enhance its adaptability to\\ndownstream tasks such as medical visual comprehension\\n(e.g., medical QA, medical dialogues, and report generation)\\nor generation tasks (e.g., super-resolution, denoising, and\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='0b816d51-7595-4e34-9554-b3965c572618', embedding=None, metadata={'page_label': '6', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 1: Comparison of HealthGPT with other LVLMs and unified multi-modal models on medical visual comprehension\\ntasks. Bold and underlined text indicates the best performance and second-best performance, respectively.\\nVQA-RAD↑ SLAKE↑ PathVQA↑Type Model # Params Medical\\nLVLM close all close all close all\\nMMMU\\n-Med ↑ OMVQA↑ Avg.↑\\nComp. Only\\nMed-Flamingo 8.3B ✓ 58.6 43.0 47.0 25.5 61.9 31.3 28.7 34.9 41.4\\nLLaV A-Med 7B ✓ 60.2 48.1 58.4 44.8 62.3 35.7 30.0 41.3 47.6\\nHuatuoGPT-Vision 7B ✓ 66.9 53.0 59.8 49.1 52.9 32.0 42.0 50.0 50.7\\nBLIP-2 6.7B ✗ 43.4 36.8 41.6 35.3 48.5 28.8 27.3 26.9 36.1\\nLLaV A-v1.5 7B ✗ 51.8 42.8 37.1 37.7 53.5 31.4 32.7 44.7 41.5\\nInstructBLIP 7B ✗ 61.0 44.8 66.8 43.3 56.0 32.3 25.3 29.0 44.8\\nYi-VL 6B ✗ 52.6 42.1 52.4 38.4 54.9 30.9 38.0 50.2 44.9\\nInternVL2 8B ✗ 64.9 49.0 66.6 50.1 60.0 31.9 43.3 54.5 52.5\\nLlama-3.2 11B ✗ 68.9 45.5 72.4 52.1 62.8 33.6 39.3 63.2 54.7\\nComp. & Gen.\\nShow-o 1.3B ✗ 50.6 33.9 31.5 17.9 52.9 28.2 22.7 45.7 42.6\\nUnified-IO 2 7B ✗ 46.2 32.6 35.9 21.9 52.5 27.0 25.3 33.0 33.8\\nJanus 1.3B ✗ 70.9 52.8 34.7 26.9 51.9 27.9 30.0 26.8 33.5\\nHealthGPT-M3 3.8B ✓ 73.7 55.9 74.6 56.4 78.7 39.7 43.3 68.5 61.3\\nHealthGPT-L14 14B ✓ 77.7 58.3 76.4 64.5 85.9 44.4 49.2 74.4 66.4\\nTable 2: The experimental results for the four modality conversion tasks.\\nCT to MRI (Brain) CT to MRI (Pelvis) MRI to CT (Brain) MRI to CT (Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\npix2pix 71.09 32.65 36.85 59.17 31.02 51.91 78.79 33.85 28.33 72.31 32.98 36.19\\nCycleGAN 54.76 32.23 40.56 54.54 30.77 55.00 63.75 31.02 52.78 50.54 29.89 67.78\\nBBDM 71.69 32.91 34.44 57.37 31.37 48.06 86.40 34.12 26.61 79.26 33.15 33.60\\nVmanba 69.54 32.67 36.42 63.01 31.47 46.99 79.63 34.12 26.49 77.45 33.53 31.85\\nDiffMa 71.47 32.74 35.77 62.56 31.43 47.38 79.00 34.13 26.45 78.53 33.68 30.51\\nHealthGPT-M3 79.38 33.03 33.48 71.81 31.83 43.45 85.06 34.40 25.49 84.23 34.29 27.99\\nHealthGPT-L14 79.73 33.10 32.96 71.92 31.87 43.09 85.31 34.29 26.20 84.96 34.14 28.13\\nmodality conversion). Notably, by this stage, the word em-\\nbedding layer and output head have been fine-tuned, only the\\nH-LoRA modules and adapter modules need to be trained.\\nThis strategy significantly improves the model’s adaptability\\nand flexibility across different tasks.\\n5 Experiments\\n5.1 Data and Experimental Setup\\nData Details. We curate VL-Health dataset (see Fig-\\nure 4). For medical visual comprehension, we leverage\\nmultiple medical-specific datasets, including PubMedVi-\\nsion (Chen et al. 2024a), LLaV A-Med (Li et al. 2024b),\\nPathVQA (He et al. 2020), MIMIC-CXR-VQA (Bae et al.\\n2024), SLAKE (Liu et al. 2021), and VQA-RAD (Lau\\net al. 2018). Additionally, we incorporate high-quality open-\\nworld data from LLaV A-1.5 (Liu et al. 2024b) to preserve\\nthe model’s general knowledge and instruction-following\\ncapabilities. For generation tasks, we construct a recon-\\nstruction dataset based on LLaV A-558k (Liu et al. 2024b),\\nand also explore two key tasks in personalized medical\\nimage enhancement—super-resolution and modality con-\\nversion—using the IXI (Davies et al. 2014) and Syn-\\nthRAD2023 (Thummerer et al. 2023) datasets. Detailed data\\nselection and instruction templates are in the Appendix.\\nModel Details. We select CLIP-L/14 (Radford et al. 2021)\\nas the visual encoder and used the hidden states of its\\nsecond and penultimate layers as concrete-grained and\\nabstract-grained features for model’s dynamic hierarchical\\nvisual perception. Drawing on the successful experiences of\\nLLaV A, we employ a MLP to align the multi-modal fea-\\nture embeddings. We choose the parameter-efficient phi-3-\\nmini (Abdin et al. 2024) and phi-4 (Abdin et al. 2024) as the\\nbase model. For visual comprehension and generation tasks,\\nwe set the rank of H-LoRA to 16 and 64, with four experts.\\nAdditionally, we use the f8-8192 version of VQGAN as the\\nimage indexing and upsampling module.\\n5.2 Main Experiments\\nComprehension. We compare HealthGPT with several\\nexisting models, including medical-specific LVLMs (e.g.,\\nMed-Flamingo (Moor et al. 2023), LLaV A-Med (Li et al.\\n2024b), HuatuoGPT-Vision (Chen et al. 2024a)) as well\\nas recent open-world LVLMs (e.g., BLIP-2 (Li et al.\\n2023b), LLaV A-v1.5 (Liu et al. 2024b), InstructBLIP (Dai\\net al. 2023), Yi-VL (Young et al. 2024), InternVL2 (Chen\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='2d851225-713e-464e-9557-39902851979e', embedding=None, metadata={'page_label': '7', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 3: Comparison results of super-resolution task.\\nModel SSIM↑ PSNR↑ MSE↓ LPIPS↓\\nSRGAN 71.34 32.01 41.27 24.50\\nDASR 71.57 32.34 38.25 19.17\\nReal-ESRGAN 67.30 31.87 42.57 20.64\\nLIIF 73.27 32.13 40.14 22.93\\nBSRGAN 69.97 31.97 41.52 28.72\\nHealthGPT-M3 78.19 32.76 34.47 12.02\\nHealthGPT-L14 77.94 32.71 35.19 12.43\\nFigure 5: Performance comparison of LoRA, MoELoRA,\\nand H-LoRA under different rank settings.\\net al. 2024b), Llama-3.2 (Dubey et al. 2024)). Addition-\\nally, we test several SOTA unified visual comprehension\\nand generation models, including Show-o (Xie et al. 2024),\\nUnified-IO 2 (Lu et al. 2024), and Janus (Wu et al.\\n2024). The experimental results are shown in Table 1, with\\nthe following key observations: (i) SOTA Results Com-\\npared with LVLMs: In medical visual comprehension\\ntasks, HealthGPT demonstrates superior performance,\\nsignificantly outperforming both medical-specific models\\n(e.g., HuatuoGPT-Vision) and general-purpose models (e.g.,\\nLlama-3.2). (ii) Surpassing Current Unified LVLMs: De-\\nspite being trained on billions of data points, unified mod-\\nels still exhibit poor generalization performance in medi-\\ncal visual comprehension. For instance, Unified-IO 2 scored\\nonly 33.8. In contrast,HealthGPT-M3, with only 3.8B pa-\\nrameters, scored 61.3 on the medical multi-modal unified\\ntask, significantly outperforming existing unified models in\\nmedical downstream scenarios. (iii) Stable Improvement\\nwith Large Base Model: Our method demonstrates excel-\\nlent scalability, with HealthGPT-L14 achieving a score\\nof 66.4 in the larger model configuration. This result signif-\\nicantly outperforms all other models, highlighting the effec-\\ntiveness of scaling up the base model for enhanced perfor-\\nmance in medical tasks.\\nGeneration. We study three key tasks in medical imag-\\ning. (i) Modality Conversion: In this task, we focus on\\nthe conversion between CT and MRI modalities for the\\nbrain and pelvic regions, designing four specific sub-tasks.\\nAll comparative models (Pix2Pix (Isola et al. 2017), Cy-\\ncleGAN (Zhu et al. 2017), BBDM (Li et al. 2023a),\\n(%)\\n(%)\\n（a） （b）\\nFigure 6: The loss visualization (a) and performance com-\\nparison (b) with respect to different visual perceptions.\\nVmamba (Liu et al. 2024e), and DiffMa (Wang et al.\\n2024b)) trained a separate model for each sub-task, while\\nHealthGPT unify all tasks into a single training process.\\nThe experimental results, shown in Table 11, demonstrate\\nthat our approach outperforms other methods across multi-\\nple evaluation metrics. For instance, in the CT2MRI-Brain\\ntask, HealthGPT-M3 achieves an SSIM of 79.38, signif-\\nicantly surpassing traditional methods like Pix2Pix (71.09)\\nand the recent DiffMa (71.47). (ii) Super-Resolution: We\\nconduct 4× super-resolution experiments on the IXI dataset,\\nwith the results presented in Table 3. Notably, most exist-\\ning methods fail to fully leverage the prior knowledge of key\\nstructures in medical images, resulting in significant short-\\ncomings in detail recovery. In contrast, our method signif-\\nicantly mitigates this issue. Specifically, HealthGPT-M3\\nexcels in key metrics such as SSIM, PSNR, and ISE, achiev-\\ning scores of 78.19, 32.76, and 34.47, respectively. Ad-\\nditionally, HealthGPT-M3 achieves the lowest score of\\n12.34, further validating its exceptional performance in hu-\\nman visual perception. (iii) Reconstruction: We compare\\nHealthGPT-M3 with unified models with reconstruction\\ncapabilities, such as Unified-IO 2 and SEED-X. The results\\nshow that our approach performs better controllability for vi-\\nsual reconstruction. We also trainHealthGPT-L14 with a\\nsimilar number of trainable parameters to the M3 version.\\nHence, the similar performance between the two models\\nmeets our expectations. Details are in the Appendix.\\n5.3 In-Depth Study\\nEffect of Heterogeneous Low-Rank Adaptation.H-LoRA\\nprovides an optimized multi-LoRA architecture for multi-\\ntask learning. We conduct extensive validation of this struc-\\nture, with results presented in Table 4, comparing the per-\\nformance of LoRA, MoELoRA, and H-LoRA in medical\\nunified comprehension and generation tasks. In the majority\\nof comprehension tasks and all generation tasks, H-LoRA\\ndemonstrates superior performance, particularly in the Om-\\nniMedVQA benchmark, where it improved from 64.90 to\\n68.50. Notably, despite some applications of MoELoRA in\\ncertain scenarios, it do not show advantages in this task and\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='ad3e1b0c-d3ab-462f-835d-4efe1468ae4c', embedding=None, metadata={'page_label': '8', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 4: We present the performance and speed differences of LoRA, MoELoRA (n=4), and H-LoRA (n=4) on medical visual\\ncomprehension and generation tasks.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQAModel\\nclose all close all close all\\nMMMU\\n-Med OMVQA RECOM MTRANS SR\\nTraining\\nTime\\nHealthGPT w/\\n+LoRA 71.3 57.2 70.0 53.4 76.4 38.6 41.30 65.10 62.67 59.99 65.88 1.00×\\n+MoELoRA 72.5 57.2 66.4 52.4 73.2 36.0 39.30 64.90 67.31 59.76 65.91 1.49×\\n+H-LoRA 73.7 55.9 74.6 56.4 78.7 39.7 43.30 68.50 67.69 60.30 66.14 1.00×\\nTable 5: Comparison between the H-LoRA-based Three-Stage Learning Strategy and the mixed-training approach.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQA CT MRITraining Strategy\\nclose all close all close all\\nMMMU\\n-Med OMVQA Brain Pelvis Brain Pelvis\\nMixed-Training 56.6 37.9 45.0 32.9 65.7 33.6 44.0 48.9 65.64 62.75 56.61 50.77HealthGPT w/ 3-stage-Training 72.5 55.2 77.9 59.6 79.7 49.0 42.7 68.5 70.84 72.99 65.26 61.33\\nFigure 7: Case study of report-to-CXR under different instructions. (a) shows a normal CXR image for comparison. (b) and (c)\\nillustrate generated cases with varying severity and affected regions. The graffiti areas indicate abnormal conditions.\\nhad a training time approximately 50% longer than LoRA.\\nFigure 5 illustrates the performance of the three PEFT meth-\\nods in medical visual comprehension and generation tasks\\nacross different ranks, with H-LoRA consistently outper-\\nforming the other methods in all scenarios, demonstrating\\nsignificant advantages in handling diverse tasks.\\nDifferent Learning Strategy. We propose a three-stage\\nlearning strategy for H-LoRA that decouples comprehension\\nand generation tasks. Unlike methods that train both tasks\\nsimultaneously, our approach reduces performance degra-\\ndation from task conflicts (see Table 5). In the medical vi-\\nsual comprehension task, mixed training causes catastrophic\\nforgetting and degrades visual reconstruction, whereas our\\nstrategy effectively uses the medical embedding knowledge\\nin pre-trained LLMs to mitigate these conflicts. Meanwhile,\\nwe examine how fusing heterogeneous H-LoRA plugins in\\nthe second training stage results in minimal performance\\ndegradation. Detailed results are in the Appendix.\\nHierarchical Visual Perception Analysis. We conduct an\\nablation analysis on visual perceptual inputs for comprehen-\\nsion and generation tasks. Figure 6 shows that comprehen-\\nsion tasks converge more efficiently with abstract-grained\\ninputs, while generation tasks perform better with concrete-\\ngrained inputs. This highlights the importance of the hier-\\narchical visual perception we propose, suggesting that tai-\\nloring visual inputs for specific tasks at different hierarchies\\ncan significantly improve efficiency.\\nReport-to-CXR Task. We further explore the medical im-\\nage generation task without reference images, using a small\\namount of MIMIC-CXR data (Johnson et al. 2019) for in-\\nstruction fine-tuning. Figure 7 annotates images with vary-\\ning injury degrees and locations, comparing them to healthy\\nCXR images. We observe thatHealthGPT effectively gen-\\nerates CXR images based on the instructions, showcasing its\\npotential in healthcare education and auxiliary diagnosis.\\n6 Conclusion\\nHealthGPT introduces an innovative PEFT approach by\\nintegrating collaborative and competitive modules, which\\nsignificantly improves the efficiency and effectiveness of\\nmulti-task learning. In the proposed CME benchmark tests,\\nHealthGPT not only achieves faster response speed but\\nalso outperforms existing multi-LoRA architectures in per-\\nformance. Future research will further explore the game-\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='e73bb5ae-77a2-4f7b-9f93-fccf691ac46f', embedding=None, metadata={'page_label': '9', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='theoretic framework based on competition and collabora-\\ntion in multi-LoRA architectures, expanding the potential of\\nPEFT.\\nReferences\\nAbdin, M.; Aneja, J.; Behl, H.; Bubeck, S.; Eldan, R.; Gu-\\nnasekar, S.; Harrison, M.; Hewett, R. J.; Javaheripi, M.;\\nKauffmann, P.; et al. 2024. Phi-4 technical report. arXiv\\npreprint arXiv:2412.08905.\\nBae, S.; Kyung, D.; Ryu, J.; Cho, E.; Lee, G.; Kweon, S.;\\nOh, J.; JI, L.; Chang, E.; Kim, T.; et al. 2024. MIMIC-Ext-\\nMIMIC-CXR-VQA: A Complex, Diverse, And Large-Scale\\nVisual Question Answering Dataset for Chest X-ray Images.\\nChen, J.; Gui, C.; Ouyang, R.; Gao, A.; Chen, S.; Chen,\\nG. H.; Wang, X.; Zhang, R.; Cai, Z.; Ji, K.; et al.\\n2024a. Huatuogpt-vision, towards injecting medical visual\\nknowledge into multimodal llms at scale. arXiv preprint\\narXiv:2406.19280.\\nChen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.; Tong,\\nW.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024b. How far are we to\\ngpt-4v? closing the gap to commercial multimodal models\\nwith open-source suites. arXiv preprint arXiv:2404.16821.\\nChern, E.; Su, J.; Ma, Y .; and Liu, P. 2024. Anole:\\nAn open, autoregressive, native large multimodal mod-\\nels for interleaved image-text generation. arXiv preprint\\narXiv:2407.06135.\\nDai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;\\nLi, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards\\nGeneral-purpose Vision-Language Models with Instruction\\nTuning. arXiv:2305.06500.\\nDavies, R. L.; Royston, P. A.; Leung, M. S.; Haider, M. E.\\nA. M. J.; Barkhof, S. G. A. L.; and B., P. E. T. M. 2014. The\\nIXI Dataset. Accessed: 2025-01-30.\\nDing, N.; Qin, Y .; Yang, G.; Wei, F.; Yang, Z.; Su, Y .;\\nHu, S.; Chen, Y .; Chan, C.-M.; Chen, W.; et al. 2023.\\nParameter-efficient fine-tuning of large-scale pre-trained\\nlanguage models. Nature Machine Intelligence, 5(3): 220–\\n235.\\nDong, R.; Han, C.; Peng, Y .; Qi, Z.; Ge, Z.; Yang, J.; Zhao,\\nL.; Sun, J.; Zhou, H.; Wei, H.; et al. 2023. Dreamllm:\\nSynergistic multimodal comprehension and creation. arXiv\\npreprint arXiv:2309.11499.\\nDubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.;\\nLetman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.;\\net al. 2024. The llama 3 herd of models. arXiv preprint\\narXiv:2407.21783.\\nEsser, P.; Rombach, R.; and Ommer, B. 2021. Taming trans-\\nformers for high-resolution image synthesis. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern\\nrecognition, 12873–12883.\\nGe, Y .; Ge, Y .; Zeng, Z.; Wang, X.; and Shan, Y . 2023. Plant-\\ning a seed of vision in large language model. arXiv preprint\\narXiv:2307.08041.\\nGe, Y .; Zhao, S.; Zhu, J.; Ge, Y .; Yi, K.; Song, L.; Li, C.;\\nDing, X.; and Shan, Y . 2024. Seed-x: Multimodal models\\nwith unified multi-granularity comprehension and genera-\\ntion. arXiv preprint arXiv:2404.14396.\\nHe, X.; Zhang, Y .; Mou, L.; Xing, E.; and Xie, P. 2020.\\nPathvqa: 30000+ questions for medical visual question an-\\nswering. arXiv preprint arXiv:2003.10286.\\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\\nof large language models. arXiv preprint arXiv:2106.09685.\\nHu, Y .; Li, T.; Lu, Q.; Shao, W.; He, J.; Qiao, Y .; and Luo,\\nP. 2024. Omnimedvqa: A new large-scale comprehensive\\nevaluation benchmark for medical lvlm. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 22170–22183.\\nIsola, P.; Zhu, J.-Y .; Zhou, T.; and Efros, A. A. 2017. Image-\\nto-image translation with conditional adversarial networks.\\nIn Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 1125–1134.\\nJohnson, A. E.; Pollard, T. J.; Greenbaum, N. R.; Lungren,\\nM. P.; Deng, C.-y.; Peng, Y .; Lu, Z.; Mark, R. G.; Berkowitz,\\nS. J.; and Horng, S. 2019. MIMIC-CXR-JPG, a large pub-\\nlicly available database of labeled chest radiographs. arXiv\\npreprint arXiv:1901.07042.\\nLau, J. J.; Gayen, S.; Ben Abacha, A.; and Demner-\\nFushman, D. 2018. A dataset of clinically generated visual\\nquestions and answers about radiology images. Scientific\\ndata, 5(1): 1–10.\\nLi, B.; Xue, K.; Liu, B.; and Lai, Y .-K. 2023a. Bbdm: Image-\\nto-image translation with brownian bridge diffusion models.\\nIn Proceedings of the IEEE/CVF conference on computer\\nvision and pattern Recognition, 1952–1961.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024a. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024b. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b. Blip-2:\\nBootstrapping language-image pre-training with frozen im-\\nage encoders and large language models. In International\\nconference on machine learning, 19730–19742. PMLR.\\nLin, T.; Liu, J.; Zhang, W.; Li, Z.; Dai, Y .; Li, H.; Yu, Z.; He,\\nW.; Li, J.; Jiang, H.; et al. 2024. Teamlora: Boosting low-\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='61b79e41-c04e-457c-ae45-4bc131935512', embedding=None, metadata={'page_label': '10', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='rank adaptation with expert collaboration and competition.\\narXiv preprint arXiv:2408.09856.\\nLiu, B.; Zhan, L.-M.; Xu, L.; Ma, L.; Yang, Y .; and Wu,\\nX.-M. 2021. Slake: A semantically-labeled knowledge-\\nenhanced dataset for medical visual question answering. In\\n2021 IEEE 18th International Symposium on Biomedical\\nImaging (ISBI), 1650–1654. IEEE.\\nLiu, D.; Zhao, S.; Zhuo, L.; Lin, W.; Qiao, Y .; Li, H.; and\\nGao, P. 2024a. Lumina-mgpt: Illuminate flexible photore-\\nalistic text-to-image generation with multimodal generative\\npretraining. arXiv preprint arXiv:2408.02657.\\nLiu, H.; Li, C.; Li, Y .; and Lee, Y . J. 2024b. Improved\\nbaselines with visual instruction tuning. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 26296–26306.\\nLiu, H.; Li, C.; Li, Y .; Li, B.; Zhang, Y .; Shen, S.; and Lee,\\nY . J. 2024c. LLaV A-NeXT: Improved reasoning, OCR, and\\nworld knowledge. https://llava-vl.github.io/blog/2024-01-\\n30-llava-next/.\\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual Instruc-\\ntion Tuning. In NeurIPS.\\nLiu, Q.; Wu, X.; Zhao, X.; Zhu, Y .; Xu, D.; Tian, F.; and\\nZheng, Y . 2024d. When moe meets llms: Parameter efficient\\nfine-tuning for multi-task medical applications. In Proceed-\\nings of the 47th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval, 1104–\\n1114.\\nLiu, Y .; Tian, Y .; Zhao, Y .; Yu, H.; Xie, L.; Wang, Y .; Ye,\\nQ.; and Liu, Y . 2024e. VMamba: Visual State Space Model.\\narXiv preprint arXiv:2401.10166.\\nLu, J.; Clark, C.; Lee, S.; Zhang, Z.; Khosla, S.; Marten, R.;\\nHoiem, D.; and Kembhavi, A. 2024. Unified-IO 2: Scaling\\nAutoregressive Multimodal Models with Vision Language\\nAudio and Action. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , 26439–\\n26455.\\nLu, J.; Clark, C.; Zellers, R.; Mottaghi, R.; and Kembhavi,\\nA. 2022. Unified-io: A unified model for vision, language,\\nand multi-modal tasks. In The Eleventh International Con-\\nference on Learning Representations.\\nLuo, T.; Lei, J.; Lei, F.; Liu, W.; He, S.; Zhao, J.; and Liu,\\nK. 2024a. Moelora: Contrastive learning guided mixture of\\nexperts on parameter-efficient fine-tuning for large language\\nmodels. arXiv preprint arXiv:2402.12851.\\nLuo, Y .; Zhang, J.; Fan, S.; Yang, K.; Hong, M.; Wu, Y .;\\nQiao, M.; and Nie, Z. 2024b. Biomedgpt: An open multi-\\nmodal large language model for biomedicine. IEEE Journal\\nof Biomedical and Health Informatics.\\nMasoudnia, S.; and Ebrahimpour, R. 2014. Mixture of ex-\\nperts: a literature survey. Artificial Intelligence Review, 42:\\n275–293.\\nMoor, M.; Huang, Q.; Wu, S.; Yasunaga, M.; Dalmia, Y .;\\nLeskovec, J.; Zakka, C.; Reis, E. P.; and Rajpurkar, P. 2023.\\nMed-flamingo: a multimodal medical few-shot learner. In\\nMachine Learning for Health (ML4H), 353–367. PMLR.\\nNath, V .; Li, W.; Yang, D.; Myronenko, A.; Zheng, M.; Lu,\\nY .; Liu, Z.; Yin, H.; Law, Y . M.; Tang, Y .; et al. 2024. Vila-\\nm3: Enhancing vision-language models with medical expert\\nknowledge. arXiv preprint arXiv:2411.12915.\\nOpenAI. 2023. GPT-4V(ision) System Card. https://cdn.\\nopenai.com/papers/GPTV System Card.pdf.\\nPan, K.; Tang, S.; Li, J.; Fan, Z.; Chow, W.; Yan, S.;\\nChua, T.-S.; Zhuang, Y .; and Zhang, H. 2024. Auto-\\nEncoding Morph-Tokens for Multimodal LLM. arXiv\\npreprint arXiv:2405.01926.\\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\\net al. 2021. Learning transferable visual models from nat-\\nural language supervision. In International conference on\\nmachine learning, 8748–8763. PMLR.\\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\\net al. 2023. Large language models encode clinical knowl-\\nedge. Nature, 620(7972): 172–180.\\nTeam, C. 2024. Chameleon: Mixed-modal early-fusion\\nfoundation models. arXiv preprint arXiv:2405.09818.\\nThawkar, O.; Shaker, A.; Mullappilly, S. S.; Cholakkal,\\nH.; Anwer, R. M.; Khan, S.; Laaksonen, J.; and Khan,\\nF. S. 2023. Xraygpt: Chest radiographs summarization\\nusing medical vision-language models. arXiv preprint\\narXiv:2306.07971.\\nThummerer, A.; van der Bijl, E.; Galapon Jr, A.; Verhoeff,\\nJ. J.; Langendijk, J. A.; Both, S.; van den Berg, C. N. A.;\\nand Maspero, M. 2023. SynthRAD2023 Grand Challenge\\ndataset: Generating synthetic CT for radiotherapy. Medical\\nphysics, 50(7): 4664–4674.\\nTian, D.; Jiang, S.; Zhang, L.; Lu, X.; and Xu, Y . 2023. The\\nrole of large language models in medical image processing:\\na narrative review. Quantitative Imaging in Medicine and\\nSurgery, 14(1): 1108.\\nTong, S.; Fan, D.; Zhu, J.; Xiong, Y .; Chen, X.; Sinha, K.;\\nRabbat, M.; LeCun, Y .; Xie, S.; and Liu, Z. 2024. Meta-\\nMorph: Multimodal Understanding and Generation via In-\\nstruction Tuning. arXiv preprint arXiv:2412.14164.\\nTu, T.; Azizi, S.; Driess, D.; Schaekermann, M.; Amin, M.;\\nChang, P.-C.; Carroll, A.; Lau, C.; Tanno, R.; Ktena, I.; et al.\\n2024. Towards generalist biomedical AI. NEJM AI, 1(3):\\nAIoa2300138.\\nVig, J. 2019. A multiscale visualization of attention in the\\ntransformer model. arXiv preprint arXiv:1906.05714.\\nWang, X.; Zhang, X.; Luo, Z.; Sun, Q.; Cui, Y .; Wang, J.;\\nZhang, F.; Wang, Y .; Li, Z.; Yu, Q.; et al. 2024a. Emu3:\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='452e4e98-58b3-4164-a927-7644ae24bd63', embedding=None, metadata={'page_label': '11', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Next-token prediction is all you need. arXiv preprint\\narXiv:2409.18869.\\nWang, Z.; Wu, Z.; Agarwal, D.; and Sun, J. 2022. Medclip:\\nContrastive learning from unpaired medical images and text.\\narXiv preprint arXiv:2210.10163.\\nWang, Z.; Zhang, L.; Wang, L.; and Zhang, Z. 2024b. Soft\\nMasked Mamba Diffusion Model for CT to MRI Conver-\\nsion. arXiv preprint arXiv:2406.15910.\\nWu, C.; Chen, X.; Wu, Z.; Ma, Y .; Liu, X.; Pan, Z.; Liu, W.;\\nXie, Z.; Yu, X.; Ruan, C.; and Luo, P. 2024. Janus: Decou-\\npling Visual Encoding for Unified Multimodal Understand-\\ning and Generation. arXiv:2410.13848.\\nWu, S.; Fei, H.; Qu, L.; Ji, W.; and Chua, T.-S. 2023.\\nNext-gpt: Any-to-any multimodal llm. arXiv preprint\\narXiv:2309.05519.\\nXie, J.; Mao, W.; Bai, Z.; Zhang, D. J.; Wang, W.; Lin, K. Q.;\\nGu, Y .; Chen, Z.; Yang, Z.; and Shou, M. Z. 2024. Show-o:\\nOne single transformer to unify multimodal understanding\\nand generation. arXiv preprint arXiv:2408.12528.\\nYoung, A.; Chen, B.; Li, C.; Huang, C.; Zhang, G.; Zhang,\\nG.; Li, H.; Zhu, J.; Chen, J.; Chang, J.; et al. 2024.\\nYi: Open foundation models by 01. ai. arXiv preprint\\narXiv:2403.04652.\\nZhou, H.; Liu, F.; Gu, B.; Zou, X.; Huang, J.; Wu, J.; Li,\\nY .; Chen, S. S.; Zhou, P.; Liu, J.; et al. 2023. A survey of\\nlarge language models in medicine: Progress, application,\\nand challenge. arXiv preprint arXiv:2311.05112.\\nZhu, J.-Y .; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-\\npaired image-to-image translation using cycle-consistent ad-\\nversarial networks. InProceedings of the IEEE international\\nconference on computer vision, 2223–2232.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='1f5acdc9-effd-4d8a-b51c-da46edc282a6', embedding=None, metadata={'page_label': '12', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix\\nThis is the Appendix for “HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation\\nvia Heterogeneous Knowledge Adaptation”. This Appendix is organized as follows:\\n• Section A presents the experimental implementation details, the training process of HealthGPT, and the specifics of\\nVL-Health.\\n• Section B systematically provides an analysis of Heterogeneous Low-Rank Adaptation.\\n• Section C shows supplementary experimental results to validate the effectiveness ofHealthGPT.\\nA Implementation Details\\nA.1 Model Details\\nWe employ CLIP-L/14 (Radford et al. 2021) as the visual feature extractor, extracting both shallow and deep features to serve as\\nvisual tokens. The model uses alignment adapters, implemented with two-layer MLPs, to align shallow features, representing\\nconcrete visual granularity, and deep features, representing abstract visual granularity. These visual tokens are concatenated\\nwith text tokens and input into the large language models (LLMs).\\nHealthGPT offers two versions: HealthGPT-M3 and HealthGPT-L14, which are based on Phi-3-mini (Abdin et al.\\n2024) and Phi-4 (Abdin et al. 2024) as the pre-trained LLMs, respectively. In addition, we expand the LLM vocabulary with\\n8192 VQ indices derived from VQGAN-f8-8192 (Esser, Rombach, and Ommer 2021), serving as multi-modal tokens to further\\naugment the model’s capacity for understanding both visual and textual input. Figure 6 shows the details.\\nTable 6: Overview of the Components of HealthGPT.\\nModel ViT Adapter MLP-dims Model dims LLM Params Vocab Size H-LoRA Rank\\nHealthGPT-M3 CLIP-L/14 2-layer MLP 1024 3072 Phi-3-mini 3.8B 40206 16(Comp.), 64(Gen.)\\nHealthGPT-L14 CLIP-L/14 2-layer MLP 1024 5120 Phi-4 14B 108547 8(Comp.), 32(Gen.)\\nA.2 Training Details\\nIn this study, we propose a three-stage learning strategy that is compatible with our innovative heterogeneous low-rank adapta-\\ntion (H-LoRA). We provide a detailed hyperparameter configuration for the model’s three-stage training process. The specific\\nhyperparameter settings used are listed in Table 7. These hyperparameters are crucial for ensuring the model’s learning efficacy\\nand final performance.\\nTable 7: Overview of Hyperparameter Configurations.\\nHealthGPT-M3 HealthGPT-L14\\nStage-1 Stage-2 Stage-3 Stage-1 Stage-2 Stage-3Hyperparameter\\nComp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen.\\nOptimizer AdamW AdamW AdamW AdamW AdamW AdamW\\nAdapter LR 1e-3 2e-5 2e-5 2e-5 1e-3 2e-5 2e-5 2e-5\\nLearning Rate / 2e-4 2e-4 2e-4 / 1e-4 2e-4 2e-4\\nGlobal Batch Size 256 64 32 128 64 256 64 32 128 64\\nWeight Decay 0 0 0 0 0 0\\nDropout Rate 0 0.05 0.05 0.05 0 0.05 0.05 0.05\\nLR Scheduler Warm Up Constant Warm Up Warm Up Constant Warm Up\\nMax Sequence Length 2048 2048 2048 2048 2048 2048\\nIt is worth noting that we sometimes observe instances of loss spikes during the training of medical visual comprehension\\nand generation tasks. Through repeated validation, we discovered that larger model parameters and learning rates tend to lead to\\nthis issue, which is the reason for the slight differences in hyperparameters betweenHealthGPT-M3 and HealthGPT-L14.\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='d2cfa991-4ab9-4947-a26a-fa0699d2fabe', embedding=None, metadata={'page_label': '13', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='（a） （b）\\nFigure 8: VL-Health dataset collection distribution.\\nA.3 VL-Health\\nThe construction of theVL-Health dataset involves two key steps:(i) data collection, (ii) data processing, as detailed below:\\nData Collection: During the collection phase, we carefully considered the diversity of medical images and the complexity of\\nthe tasks, selecting appropriate subsets for comprehension and generation tasks. For comprehension tasks, we selected datasets\\nsuch as VQA-RAD (Lau et al. 2018), SLAKE (Liu et al. 2021), PathVQA (He et al. 2020), and MIMIC-CXR-VQA (Bae et al.\\n2024), which cover various medical imaging modalities like radiology and pathology, and include professional annotations\\nto assist the model in learning tasks such as lesion detection and disease diagnosis. Additionally, large-scale multi-modal\\ndatasets like LLaV A-Med (Li et al. 2024b) and PubMedVision (Chen et al. 2024a) were included to provide broader medical\\nknowledge support and facilitate the training of complex reasoning tasks. For generation tasks, we focused on four mainstream\\ntask categories: super-resolution image generation, modality conversion, text-to-image generation, and image reconstruction.\\nThe IXI (Davies et al. 2014) dataset, containing a large number of healthy brain MRI images, is suitable for training super-\\nresolution models; the MIMIC-CHEST-XRAY (Bae et al. 2024) dataset, with X-ray images and their corresponding textual\\nreports, is appropriate for text-to-image generation tasks; the SynthRAD2023 (Thummerer et al. 2023) dataset provides a large\\nnumber of paired CT and MRI images, supporting modality conversion model training; for image reconstruction tasks, we\\nrewrote and adjusted the LLaV A-558k (Liu et al. 2024b) dataset.\\nData Processing: After data collection, we performed filtering and processing of the raw data. For VisualQA tasks, we stan-\\ndardized the data entries into two forms: open-ended questions and single-choice questions, enabling flexible training and\\nevaluation. Additionally, considering that multi-image data has a minimal impact on performance but introduces extra padding\\nand training time, we excluded multi-image data. For the scanned image data in generation tasks, we applied slicing extrac-\\ntion, image registration, data augmentation, and normalization to treat 2D images as visual inputs for model training or used\\nVQGAN-generated indices to supervise the generation tasks.\\nData Statistics This section provides detailed statistical information about the VL-Health dataset to offer a more compre-\\nhensive understanding.\\nData Overview: To ensure a balanced development of the model’s comprehension and generation capabilities, in addition\\nto the LLaV A-558k and PubMedVision-PT datasets used for alignment, the VL-Health dataset ultimately selected 765,802\\nadditional visual question-answering (VQA) training samples (to endow the model with visual comprehension and instruction-\\nfollowing capabilities) and 783,045 generation training samples (to provide the model with reconstruction and visual generation\\ninstruction-following abilities). This contributes to the transfer of knowledge between comprehension and generation tasks, en-\\nhancing the model’s overall performance. For medical image comprehension tasks, images were selected from VQA-RAD (ap-\\nproximately 450 images), SLAKE (approximately 630 images), PathVQA (approximately 2,600 images), MIMIC-CXR-VQA\\n(approximately 52,000 images), LLaV A-Med (approximately 61,000 images), and PubMedVision (approximately 500,000 im-\\nages). Multiple question-answer pairs were retained for each image to enhance the model’s understanding and generalization\\nof the image content. Table 8 shows the data distribution ofVL-Health for three-stage learning strategy, where mixed-47k is\\nbased on the sampling of all data in stage-1.\\nDiversity and Quality Assessment: VL-Health covers 11 modalities, including CT, MRI, X-ray, microscopy, OCT, ultra-\\nsound, and fundus photography, which aids the model in learning features from various modalities. The dataset also encom-\\npasses a wide range of diseases, from common to rare, and from localized lesions to systemic diseases, including pulmonary\\ndiseases, skeletal abnormalities, brain lesions, tumors, cardiovascular diseases, and cellular abnormalities. This provides com-\\nprehensive training support to the model, enabling it to learn the characteristics and diagnosis of various diseases.\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='78885648-63f7-47da-9b7a-e7ce735f7882', embedding=None, metadata={'page_label': '14', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8: Data distribution of VL-Health in three-stage learning strategy.\\nMedical Task Stage-1 Stage-2\\nComp. LLaV A-558k, PubMedVision-PT Mixed-47kGen. LLaV A-558k\\nMedical Task Stage-3\\nComp. LLaV A Med, MIMIC CXR VQA, PubMedVision-FT, LLaV A-665k, PathVQA, SLAKE, VQA-RAD\\nGen. IXI, SynthRAD2023, MIMIC-CHEST-XRAY\\nData Format. All data samples are converted into a unified instruction-response format for training and evaluation. Specifi-\\ncally, the VL-Health dataset consists of the following components:\\n• Task Type: Specifies the granularity of visual features output by the visual encoder and selects the corresponding H-\\nLoRA submodule. For generation tasks, the response also includes multi-modal tokens corresponding to VQ indices.\\n• Task Instruction: Guides the model to interpret the image and generate a response, covering various aspects of the\\nimage and specifying the output format.\\n• Response: The textual output generated based on the task instruction and input image, ensuring it meets the question and\\nformatting requirements.\\n• Input Image: Provides the visual signal for the model to process.\\n• Target Image Index: In generation tasks, this is added as a multi-modal token to the response for autoregressive\\ngeneration.\\nB Analysis of Heterogeneous Low-Rank Adaptation\\nWe propose H-LoRA, which utilizes hard routing selection to allocate plugins for knowledge learning and representation across\\ntasks, thereby preventing conflicts arising from heterogeneous knowledge. Furthermore, within each task, we optimized based\\non MoELoRA, enhancing performance while reducing computational overhead. The pseudocode is detailed Algorithm 1.\\nAlgorithm 1: H-LoRA Algorithm\\nInput: concrete-grained visual features FCon, abstract-grained visual featuresFAbs, comprehension-based H-LoRA modules\\n({AComp.\\ni }k\\ni=1, RComp.\\nouter ), generation-based H-LoRA modules({AGen.\\ni }k\\ni=1, RGen.\\nouter), task type T (comprehension or generation),\\nnumber of LoRA experts k, origin linear layer weights W0, text features T , hidden state h\\nOutput: final output O\\n// Select task-specific image features\\nif T = generation task then\\nFimg ← FCon\\nelse if T = comprehension task then\\nFimg ← FAbs\\nend if\\nU ←concat(Fimg, T ) // Concatenate image features and text features\\n{Ai}k\\ni=1, {Bi}k\\ni=1, Router ← {AT\\ni }k\\ni=1, {BT\\ni }k\\ni=1, RT\\nouter // Assign task-specific H-LoRA submodule\\n// Merge LoRA experts’ matrices\\nAmerged ← concat({Ai}k\\ni=1)\\nBmerged ← concat({Bi}k\\ni=1)\\nW ←R(h) // Generate routing weights based on input hidden state x\\nWexpanded ← α × W/r ⊗ 1r // Expand routing weights to match merged matrices\\nOH-LoRA ← (x · Amerged ⊙ Wexpanded) · Bmerged // Compute H-LoRA output using element-wise multiplication\\nO ←x · W0 + OH-LoRA // Add H-LoRA output to pre-trained weights to get final output\\nReturn O\\nWe further analyzed the computational overhead differences between MoELoRA and H-LoRA. Assuming that both methods\\nuse the same number of LoRA experts k, we can compare their time complexity from the perspective of the operational steps\\ninvolved.\\nComputational Overhead of MoELoRA. In MoELoRA, the operations involving the expert matrix mainly include the fol-\\nlowing steps: (i) Expert Multiplication : MoELoRA requires 2k multiplications with the LoRA experts. (ii) Router Multi-\\nplication: One multiplication with the Router is required. (iii) Router Output Expansion : MoELoRA needs to perform k\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='f4d2b425-bb1f-4b30-9fa9-d0c812020776', embedding=None, metadata={'page_label': '15', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='expansion operations on the Router’s output weights to generate the appropriate shapes that match the dimensions of the in-\\nput and LoRA experts while iterating through the experts. (iv) Dot Product: For each expanded Router weight, a dot product\\nwith the intermediate state of the expert is required, resulting in k multiplications. (v) Addition: Finally, k addition operations\\nare required to accumulate the results from each LoRA expert into the final output. Assuming the time complexity of each\\noperation is the same, the additional time complexity introduced when equipping a fully connected layer with MoELoRA is:\\nO(2k+1+ k+k+k) = O(5k+1). Thus, MoELoRA introduces an additional time overhead ofO(5k+1) during computation.\\nH-LoRA. In contrast to MoELoRA, H-LoRA reduces the computational overhead by concatenating the LoRA expert matrices.\\nSpecifically: (i) Expert Multiplication: H-LoRA merges all LoRA experts by directly creating a larger A and B matrix, instead\\nof performing independent operations for each expert. This process can be implemented through matrix initialization without\\nadditional concatenation operations. Therefore, only 2 multiplications with the LoRA experts are required. (ii) Router Multi-\\nplication: H-LoRA still requires one multiplication with the Router. (iii) Router Output Expansion : H-LoRA only requires\\none expansion operation on the Router’s output weights. (iv) Dot Product: H-LoRA only requires one dot product between\\nthe Router’s output and the expert’s intermediate state. (v) Addition: Finally, H-LoRA only requires one addition operation\\nto accumulate the LoRA expert results into the intermediate state. Therefore, the additional time complexity introduced by\\nH-LoRA is: O(2 + 1 + 1 + 1 + 1) =O(6).\\nComparing the two, we see that MoELoRA introduces a linear increase in additional time complexity with respect to the\\nnumber of experts k, resulting in a complexity of O(5k + 1), while H-LoRA’s additional time complexity is fixed at O(6),\\nindependent of k. We observe that when k is small, the time complexity differences between MoELoRA and H-LoRA are\\nnegligible. However, as k increases, MoELoRA’s computational overhead grows linearly, while H-LoRA’s remains constant.\\nThis makes H-LoRA significantly more computationally efficient than MoELoRA, particularly in large-scale tasks. We will\\nfurther demonstrate the significant advantage of H-LoRA in training time in subsequent experiments, validating its efficiency\\nin practical applications.\\nC Supplemental Experimental Results\\nIn this section, we include additional experiments to demonstrate the superiority of HealthGPT and articulate our design\\nphilosophy.\\nC.1 Results: OmniMedVQA Benchmark\\nOmniMedVQA (Hu et al. 2024) is a novel, large-scale medical visual question answering (VQA) benchmark designed to\\nencompass various modalities and anatomical regions by collecting diverse images from multiple medical datasets. Our exper-\\nimental results are presented in Table 9.\\nTable 9: Performance comparison of OmniMedVQA Benchmark.\\nOmniMedVQA↑Type Model # Params Medical\\nLVLM CT X-ray FDM MiS OCT MRI USS Avg.\\nComp. Only\\nMed-Flamingo 8.3B ✓ 30.1 33.9 25.5 37.0 60.0 27.6 30.4 34.9\\nLLaV A-Med 7B ✓ 28.4 32.8 42.7 31.6 55.3 45.0 53.6 41.3\\nHuatuoGPT-Vision 7B ✓ 35.3 41.5 51.4 62.3 59.3 40.4 60.1 50.0\\nBLIP-2 6.7B ✗ 26.6 29.1 22.3 36.9 29.1 22.7 21.4 26.9\\nLLaV A-v1.5 7B ✗ 28.0 55.7 35.5 42.1 49.2 52.9 49.7 44.7\\nInstructBLIP 7B ✗ 20.1 22.2 34.1 30.6 38.6 31.9 25.5 29.0\\nYi-VL 6B ✗ 51.2 47.1 27.7 62.6 67.6 55.0 40.3 50.2\\nInternVL2 8B ✗ 40.2 57.9 53.2 64.0 59.1 58.1 49.1 54.5\\nLlama-3.2 11B ✗ 37.6 55.2 71.4 82.1 62.5 65.2 68.6 63.2\\nComp. & Gen.\\nShow-o 1.3B ✗ 29.0 50.4 30.9 22.0 30.8 34.2 33.8 33.0\\nUnified-IO 2 7B ✗ 10.8 37.7 12.3 25.3 32.6 30.9 37.7 26.8\\nJanus 1.3B ✗ 24.9 54.8 35.9 62.7 54.2 50.7 36.8 45.7\\nHealthGPT-M3 3.8B ✓ 35.3 81.9 54.6 88.2 89.3 78.5 51.4 68.5\\nHealthGPT-L14 14B ✓ 39.0 86.6 64.1 88.6 99.7 80.9 62.2 74.4\\nThrough our analysis, we make the following observations: (i) HealthGPT-M3 outperforms other models in 4 out of 7\\nsub-tasks, achieving an average score that exceeds cutting-edge medical Large Vision-Language Models (LVLMs) as well as\\n15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='3de3f1d3-39ab-4a20-bf3d-bf192723afd0', embedding=None, metadata={'page_label': '16', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='general LVLMs; (ii) the unified model demonstrates relatively weak performance on OmniMedVQA; however, our approach\\neffectively mitigates performance degradation caused by generation tasks, serving as a unified model; (iii) HealthGPT-L14\\nexcels across all sub-tasks, achieving optimal or near-optimal results with an average score of 74.4, significantly surpassing\\nother models.\\nC.2 Stability Analysis of Number of Experts\\nWe investigated the impact of the number of LoRA experts on model performance within a multi-LoRA architecture, conducting\\nextensive experiments on MoELoRA and H-LoRA with varying numbers of experts. The experimental results are presented\\nin Table 10. As the number of experts increases, the training time for MoELoRA is significantly prolonged. When n = 8 ,\\nthe training time for MoELoRA is twice that of LoRA, whereas H-LoRA incurs no additional training delay and performs\\nbetter. It is estimated that at n = 32, the training time for MoELoRA could reach eight times that of LoRA, preventing it from\\ncompleting training and inference. This result aligns with the analysis in Appendix B, indicating that H-LoRA not only avoids\\nintroducing additional training delays compared to LoRA but also outperforms MoELoRA.\\nTable 10: We explored the performance of MoELoRA and H-LoRA with different numbers of LoRA experts. At n = 32 ,\\nMoELoRA was unable to complete training.\\nn=2 n=4 n=8 n=32Model Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time\\n+MoELoRA 50.3 62.98 1.22 × 50.0 64.33 1.49 × 50.8 63.71 2.09 × / / 5.81 ×HealthGPT w/ +H-LoRA 51.5 63.48 0.99× 52.8 64.71 1.00× 53.6 64.98 0.99× 53.5 64.74 1.01×\\nC.3 Impact of Heterogeneous Knowledge Fusion on Performance\\nTraditional unified models often utilize mixed training methods, which may result in performance degradation due to variations\\nin task modes. To address this, we propose a three-phase learning strategy to support H-LoRA, effectively mitigating inter-task\\nconflicts. Specifically, the second phase (Heterogeneous H-LoRA Plugin Adaptation) integrates LLMs with different H-LoRA\\nplugins into a new unified foundation by mixing the training of the embedding layers and output heads for two tasks. Figure\\n9 illustrates the impact of this phase on the performance of medical comprehension and generation tasks. We observe that the\\nsecond phase effectively unifies the model with minimal impact on overall performance, significantly alleviating the conflict\\nissues arising from mixed training in medical scenarios.\\nC.4 Human Evaluation.\\n65.7 65.4 67.7 67.0\\nFigure 9: Performance changes before and after the\\nstage-2.\\nWe further conduct human evaluation on the VQA-RAD, SLAKE,\\nand PathVQA benchmarks, which contain 1,000 open-ended ques-\\ntions. Specifically, we recruit 5 clinicians to rank the randomly shuf-\\nfled responses from HealthGPT-L14, LLaV A-Med, HuatuoGPT-\\nVision, Llama-3.2, InternVL-2 and Show-o. During the evaluation,\\nquestions were randomly selected, and the model-generated responses\\nwere anonymized and ranked. The results, as shown in Figure 10, in-\\ndicate that HealthGPT was frequently selected as the best answer.\\nThis suggests that HealthGPT has further application potential in\\nmedical care scenarios.\\nC.5 Reconstruction Performance\\nCurrently, unified models that align visual features based on recon-\\nstruction tasks include pre-LVLMs, post-LVLMs, as well as Unified-\\nIO 2 (Lu et al. 2024) and SEED-X (Ge et al. 2024). To investigate the\\ncontrollability of visual generation in rigorous settings such as med-\\nical contexts, we evaluated the performance of these models in med-\\nical image reconstruction in Table 11. Experimental results demon-\\nstrate that HealthGPT exhibits the most stable reconstruction per-\\nformance with a small amount of data.\\n16', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='25ae8a39-3e8d-4961-b1d2-83855e3d8516', embedding=None, metadata={'page_label': '17', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='C.6 Case Study\\nFigures 11 and 12 illustrate examples of modality transformation and super-resolution reconstruction. In Figure 11, the results\\ngenerated by our method in the CT (MRI) to MRI (CT) transformation task are highly close to the ground truth, effectively\\nguiding the model in the transformation across different regions. For the MRI super-resolution reconstruction task, Figure 12\\ndemonstrates the accuracy of our method in restoring scan image details, accurately reconstructing the essential details of the\\nimage.\\nTable 11: The experimental results for the four reconstruction tasks.\\nCT(Brain) CT(Pelvis) MRI (Brain) MRI(Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\nSEED-X 20.18 27.66 112.11 21.53 28.02 102.87 4.90 27.62 112.86 6.31 27.89 106.21\\nUnified-IO 2 83.93 36.09 17.95 85.36 35.10 25.46 87.50 34.25 25.47 86.31 33.53 29.80\\nHealthGPT-M3 91.73 36.42 15.46 94.26 37.30 12.53 88.76 33.97 27.05 84.40 33.11 32.62\\n34.08\\n15.87\\n9.16\\n5.62\\n13.33\\n21.94\\nHealthGPT\\nLLaVA-Med\\nHuatuoGPT-Vision\\nLlama-3.2\\nInternVL-2\\nShow-o\\nHuman\\nEvaluation\\n(a) (b)\\nFigure 10: (a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset.\\n17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='f54e3b69-ab15-4399-a408-cabdba8a2de6', embedding=None, metadata={'page_label': '18', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 11: Case of modality transfer.\\n18', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='8e4db909-49ea-475b-a3de-a44cf538fc4f', embedding=None, metadata={'page_label': '19', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 12: Case of MRI image super-resolution.\\n19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform"
      ],
      "metadata": {
        "id": "wWVwkooyiFMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hide some keys from llm\n",
        "\n",
        "docs[0].__dict__ # too much data about one doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "axrU95QwntZH",
        "outputId": "44da32e9-a225-45c0-ad4c-d4dc13712fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': '6e6bc31c-24ec-46d9-a214-8010c98d526d',\n",
              " 'embedding': None,\n",
              " 'metadata': {'page_label': '1',\n",
              "  'file_name': '2502.09838v2.pdf',\n",
              "  'file_path': '/content/data/2502.09838v2.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 8787043,\n",
              "  'creation_date': '2025-02-23',\n",
              "  'last_modified_date': '2025-02-23'},\n",
              " 'excluded_embed_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'excluded_llm_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'relationships': {},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text_resource': MediaResource(embeddings=None, data=None, text='HealthGPT: A Medical Large Vision-Language Model for Unifying\\nComprehension and Generation via Heterogeneous Knowledge Adaptation\\nTianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\\nMengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\\n1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\\n4The Hong Kong University of Science and Technology,5National University of Singapore\\nProject Page\\n Code\\n1. X-Ray \\nComprehension\\n2. CT\\nComprehension\\n3. MRI\\nComprehension\\nComp. Perf.\\n7 Medical Multi-Modal Comprehension Tasks\\nGen. \\nPerformance\\n5 Medical Multi-Modal Generation Tasks\\nList all anatomical locations showing \\npulmonary edema, hazy opacity, or \\nmediastinal displacement.\\nLeft hilar structures, left lung,\\nright hilar structures, right lung.\\nWhich abdominal organ shows any \\nindication of a lesion or abnormality \\nin the CT image?\\nNo abdominal organs show\\nany clear indications of lesions\\nor abnormalities.\\nCould you explain what this mass\\nin the MRI means for my health? \\nIs it very serious?\\nCertainly, the MRI shows a\\ndefined mass in your left nasal\\ncavity. There is no sign of the ...\\n4. Microsopy Comprehension\\n What is the purpose of the different \\nmembrane treatments used in this \\nstudy?\\nThe purpose of the different\\nmembrane treatments used…\\n5. OCT Comprehension\\n What is the purpose of comparing \\nthe OCT structure image and OCTA \\nimage with H&E histology?\\nTo confirm the histological position\\nof the obtained OCT brain images.\\n6. Fundus\\nComprehension\\n What specific findings or pathological \\nchanges can be observed in this \\nfundus image?\\nThe fundus image appears normal with\\nno noticeable signs of pathology…\\n7. Ultrasound\\nComprehension\\n What type of imaging technique \\nis used in this image?\\nThe image is a sagittal gray-\\nscale ultrasonographic…\\n1. CT2MRI\\nGeneration\\nI need a version of this CT representation \\nin MRI.\\nThe image has\\nbeen transformed\\ninto MRI.\\n2. MRI2CT\\nGeneration\\nTransform the MRI display into a \\nCT image.\\nHere is the CT\\nversion of the\\nMRI image.\\n3. Image Reconstruction\\nReconstruct the following \\nmedical images.\\nHere is the reconstructed\\nmedical image you need.\\n4. Super Resolution\\nCould you improve the quality\\nof this MRI image?\\nHere is the image with\\nimproved resolution.\\n5. Report-to-CXR\\nThe X-ray shows no \\npleural effusion or \\npneumothorax.\\nHere is the\\nchest X-ray\\nimage for\\nyou.\\nGen. Perf.\\nFigure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\\nunified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\\nplex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\\nAbstract\\nWe present HealthGPT, a powerful Medical Large Vision-\\nLanguage Model (Med-LVLM) that integrates medical vi-\\nsual comprehension and generation capabilities within a uni-\\nfied autoregressive paradigm. Our bootstrapping philosophy\\nis to progressively adapt heterogeneous comprehension and\\ngeneration knowledge to pre-trained large language mod-\\nels (LLMs). This is achieved through a novel heterogeneous\\nlow-rank adaptation (H-LoRA) technique, which is com-\\nplemented by a tailored hierarchical visual perception ap-\\nproach and a three-stage learning strategy. To effectively\\nlearn the HealthGPT, we devise a comprehensive medi-\\ncal domain-specific comprehension and generation dataset\\ncalled VL-Health. Experimental results demonstrate ex-\\nceptional performance and scalability of HealthGPT in\\nmedical visual unified tasks. Our project can be accessed at\\nhttps://github.com/DCDmllm/HealthGPT.\\n1 Introduction\\nLarge Vision-Language Models (LVLMs) (Liu et al. 2023;\\nOpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\\ndemonstrated outstanding open-world visual comprehension\\nand reasoning abilities through language-based interactive\\ndialogue over the past years, simultaneously opening up\\nnew opportunities for applications in specialized domains.\\n1\\narXiv:2502.09838v2  [cs.CV]  17 Feb 2025', path=None, url=None, mimetype=None),\n",
              " 'image_resource': None,\n",
              " 'audio_resource': None,\n",
              " 'video_resource': None,\n",
              " 'text_template': '{metadata_str}\\n\\n{content}'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# quick example of what the LLM and Embeddings see when with a test document\n",
        "\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.schema import MetadataMode\n",
        "\n",
        "document = Document(\n",
        "    text=\"This is a super-customized document\",\n",
        "    metadata={\n",
        "        \"file_name\": \"super_secret_document.txt\",\n",
        "        \"category\": \"finance\",\n",
        "        \"author\": \"LlamaIndex\",\n",
        "    },\n",
        "    # excluded_embed_metadata_keys=[\"file_name\"],\n",
        "    excluded_llm_metadata_keys=[\"category\"],\n",
        "    metadata_seperator=\"\\n\",\n",
        "    metadata_template=\"{key}:{value}\",\n",
        "    text_template=\"Metadata:\\n{metadata_str}\\n-----\\nContent:\\n{content}\",\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"The LLM sees this: \\n\",\n",
        "    document.get_content(metadata_mode=MetadataMode.LLM),\n",
        ")\n",
        "# print(\n",
        "#     \"The Embedding model sees this: \\n\",\n",
        "#     document.get_content(metadata_mode=MetadataMode.EMBED),\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmbdVZ_mpvcH",
        "outputId": "1c25aac8-ec4e-4ecb-f538-a015291246ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLM sees this: \n",
            " Metadata:\n",
            "file_name:super_secret_document.txt\n",
            "author:LlamaIndex\n",
            "-----\n",
            "Content:\n",
            "This is a super-customized document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.schema import MetadataMode\n",
        "\n",
        "# print(docs[0].get_content(metadata_mode=MetadataMode.LLM))   # what the llm sees\n",
        "print(docs[0].get_content(metadata_mode=MetadataMode.EMBED)) # what embeddings see. in this case, same thing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kpfv9Auvn_1q",
        "outputId": "12c9eec6-dacd-4b26-bad0-54b81a510996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_label: 1\n",
            "file_path: /content/data/2502.09838v2.pdf\n",
            "\n",
            "HealthGPT: A Medical Large Vision-Language Model for Unifying\n",
            "Comprehension and Generation via Heterogeneous Knowledge Adaptation\n",
            "Tianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\n",
            "Mengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\n",
            "1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\n",
            "4The Hong Kong University of Science and Technology,5National University of Singapore\n",
            "Project Page\n",
            " Code\n",
            "1. X-Ray \n",
            "Comprehension\n",
            "2. CT\n",
            "Comprehension\n",
            "3. MRI\n",
            "Comprehension\n",
            "Comp. Perf.\n",
            "7 Medical Multi-Modal Comprehension Tasks\n",
            "Gen. \n",
            "Performance\n",
            "5 Medical Multi-Modal Generation Tasks\n",
            "List all anatomical locations showing \n",
            "pulmonary edema, hazy opacity, or \n",
            "mediastinal displacement.\n",
            "Left hilar structures, left lung,\n",
            "right hilar structures, right lung.\n",
            "Which abdominal organ shows any \n",
            "indication of a lesion or abnormality \n",
            "in the CT image?\n",
            "No abdominal organs show\n",
            "any clear indications of lesions\n",
            "or abnormalities.\n",
            "Could you explain what this mass\n",
            "in the MRI means for my health? \n",
            "Is it very serious?\n",
            "Certainly, the MRI shows a\n",
            "defined mass in your left nasal\n",
            "cavity. There is no sign of the ...\n",
            "4. Microsopy Comprehension\n",
            " What is the purpose of the different \n",
            "membrane treatments used in this \n",
            "study?\n",
            "The purpose of the different\n",
            "membrane treatments used…\n",
            "5. OCT Comprehension\n",
            " What is the purpose of comparing \n",
            "the OCT structure image and OCTA \n",
            "image with H&E histology?\n",
            "To confirm the histological position\n",
            "of the obtained OCT brain images.\n",
            "6. Fundus\n",
            "Comprehension\n",
            " What specific findings or pathological \n",
            "changes can be observed in this \n",
            "fundus image?\n",
            "The fundus image appears normal with\n",
            "no noticeable signs of pathology…\n",
            "7. Ultrasound\n",
            "Comprehension\n",
            " What type of imaging technique \n",
            "is used in this image?\n",
            "The image is a sagittal gray-\n",
            "scale ultrasonographic…\n",
            "1. CT2MRI\n",
            "Generation\n",
            "I need a version of this CT representation \n",
            "in MRI.\n",
            "The image has\n",
            "been transformed\n",
            "into MRI.\n",
            "2. MRI2CT\n",
            "Generation\n",
            "Transform the MRI display into a \n",
            "CT image.\n",
            "Here is the CT\n",
            "version of the\n",
            "MRI image.\n",
            "3. Image Reconstruction\n",
            "Reconstruct the following \n",
            "medical images.\n",
            "Here is the reconstructed\n",
            "medical image you need.\n",
            "4. Super Resolution\n",
            "Could you improve the quality\n",
            "of this MRI image?\n",
            "Here is the image with\n",
            "improved resolution.\n",
            "5. Report-to-CXR\n",
            "The X-ray shows no \n",
            "pleural effusion or \n",
            "pneumothorax.\n",
            "Here is the\n",
            "chest X-ray\n",
            "image for\n",
            "you.\n",
            "Gen. Perf.\n",
            "Figure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\n",
            "unified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\n",
            "plex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\n",
            "Abstract\n",
            "We present HealthGPT, a powerful Medical Large Vision-\n",
            "Language Model (Med-LVLM) that integrates medical vi-\n",
            "sual comprehension and generation capabilities within a uni-\n",
            "fied autoregressive paradigm. Our bootstrapping philosophy\n",
            "is to progressively adapt heterogeneous comprehension and\n",
            "generation knowledge to pre-trained large language mod-\n",
            "els (LLMs). This is achieved through a novel heterogeneous\n",
            "low-rank adaptation (H-LoRA) technique, which is com-\n",
            "plemented by a tailored hierarchical visual perception ap-\n",
            "proach and a three-stage learning strategy. To effectively\n",
            "learn the HealthGPT, we devise a comprehensive medi-\n",
            "cal domain-specific comprehension and generation dataset\n",
            "called VL-Health. Experimental results demonstrate ex-\n",
            "ceptional performance and scalability of HealthGPT in\n",
            "medical visual unified tasks. Our project can be accessed at\n",
            "https://github.com/DCDmllm/HealthGPT.\n",
            "1 Introduction\n",
            "Large Vision-Language Models (LVLMs) (Liu et al. 2023;\n",
            "OpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\n",
            "demonstrated outstanding open-world visual comprehension\n",
            "and reasoning abilities through language-based interactive\n",
            "dialogue over the past years, simultaneously opening up\n",
            "new opportunities for applications in specialized domains.\n",
            "1\n",
            "arXiv:2502.09838v2  [cs.CV]  17 Feb 2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "    # define the content/metadata template\n",
        "    doc.text_template = \"Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}\"\n",
        "\n",
        "    # exclude page label from embedding\n",
        "    if \"page_label\" not in doc.excluded_embed_metadata_keys:\n",
        "        doc.excluded_embed_metadata_keys.append(\"page_label\")"
      ],
      "metadata": {
        "id": "1bmx1ZCisotj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after editing the content seen by embedings\n",
        "\n",
        "print(docs[0].get_content(metadata_mode=MetadataMode.EMBED))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "n8hDb9WztOHm",
        "outputId": "ec7ca63a-7887-4625-e5c0-9193dc7d1489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata:\n",
            "file_path: /content/data/2502.09838v2.pdf\n",
            "---\n",
            "Content:\n",
            "HealthGPT: A Medical Large Vision-Language Model for Unifying\n",
            "Comprehension and Generation via Heterogeneous Knowledge Adaptation\n",
            "Tianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\n",
            "Mengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\n",
            "1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\n",
            "4The Hong Kong University of Science and Technology,5National University of Singapore\n",
            "Project Page\n",
            " Code\n",
            "1. X-Ray \n",
            "Comprehension\n",
            "2. CT\n",
            "Comprehension\n",
            "3. MRI\n",
            "Comprehension\n",
            "Comp. Perf.\n",
            "7 Medical Multi-Modal Comprehension Tasks\n",
            "Gen. \n",
            "Performance\n",
            "5 Medical Multi-Modal Generation Tasks\n",
            "List all anatomical locations showing \n",
            "pulmonary edema, hazy opacity, or \n",
            "mediastinal displacement.\n",
            "Left hilar structures, left lung,\n",
            "right hilar structures, right lung.\n",
            "Which abdominal organ shows any \n",
            "indication of a lesion or abnormality \n",
            "in the CT image?\n",
            "No abdominal organs show\n",
            "any clear indications of lesions\n",
            "or abnormalities.\n",
            "Could you explain what this mass\n",
            "in the MRI means for my health? \n",
            "Is it very serious?\n",
            "Certainly, the MRI shows a\n",
            "defined mass in your left nasal\n",
            "cavity. There is no sign of the ...\n",
            "4. Microsopy Comprehension\n",
            " What is the purpose of the different \n",
            "membrane treatments used in this \n",
            "study?\n",
            "The purpose of the different\n",
            "membrane treatments used…\n",
            "5. OCT Comprehension\n",
            " What is the purpose of comparing \n",
            "the OCT structure image and OCTA \n",
            "image with H&E histology?\n",
            "To confirm the histological position\n",
            "of the obtained OCT brain images.\n",
            "6. Fundus\n",
            "Comprehension\n",
            " What specific findings or pathological \n",
            "changes can be observed in this \n",
            "fundus image?\n",
            "The fundus image appears normal with\n",
            "no noticeable signs of pathology…\n",
            "7. Ultrasound\n",
            "Comprehension\n",
            " What type of imaging technique \n",
            "is used in this image?\n",
            "The image is a sagittal gray-\n",
            "scale ultrasonographic…\n",
            "1. CT2MRI\n",
            "Generation\n",
            "I need a version of this CT representation \n",
            "in MRI.\n",
            "The image has\n",
            "been transformed\n",
            "into MRI.\n",
            "2. MRI2CT\n",
            "Generation\n",
            "Transform the MRI display into a \n",
            "CT image.\n",
            "Here is the CT\n",
            "version of the\n",
            "MRI image.\n",
            "3. Image Reconstruction\n",
            "Reconstruct the following \n",
            "medical images.\n",
            "Here is the reconstructed\n",
            "medical image you need.\n",
            "4. Super Resolution\n",
            "Could you improve the quality\n",
            "of this MRI image?\n",
            "Here is the image with\n",
            "improved resolution.\n",
            "5. Report-to-CXR\n",
            "The X-ray shows no \n",
            "pleural effusion or \n",
            "pneumothorax.\n",
            "Here is the\n",
            "chest X-ray\n",
            "image for\n",
            "you.\n",
            "Gen. Perf.\n",
            "Figure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\n",
            "unified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\n",
            "plex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\n",
            "Abstract\n",
            "We present HealthGPT, a powerful Medical Large Vision-\n",
            "Language Model (Med-LVLM) that integrates medical vi-\n",
            "sual comprehension and generation capabilities within a uni-\n",
            "fied autoregressive paradigm. Our bootstrapping philosophy\n",
            "is to progressively adapt heterogeneous comprehension and\n",
            "generation knowledge to pre-trained large language mod-\n",
            "els (LLMs). This is achieved through a novel heterogeneous\n",
            "low-rank adaptation (H-LoRA) technique, which is com-\n",
            "plemented by a tailored hierarchical visual perception ap-\n",
            "proach and a three-stage learning strategy. To effectively\n",
            "learn the HealthGPT, we devise a comprehensive medi-\n",
            "cal domain-specific comprehension and generation dataset\n",
            "called VL-Health. Experimental results demonstrate ex-\n",
            "ceptional performance and scalability of HealthGPT in\n",
            "medical visual unified tasks. Our project can be accessed at\n",
            "https://github.com/DCDmllm/HealthGPT.\n",
            "1 Introduction\n",
            "Large Vision-Language Models (LVLMs) (Liu et al. 2023;\n",
            "OpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\n",
            "demonstrated outstanding open-world visual comprehension\n",
            "and reasoning abilities through language-based interactive\n",
            "dialogue over the past years, simultaneously opening up\n",
            "new opportunities for applications in specialized domains.\n",
            "1\n",
            "arXiv:2502.09838v2  [cs.CV]  17 Feb 2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are other, more advanced transformations. Some require an LLM to work. We will use Qwen 2.5 32B Instruct 128k through Groq, which is an affordble, high-rate model. It should be enough to extract Q&As and titles from the documents."
      ],
      "metadata": {
        "id": "iflpVVGqzN4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq llama-index-llms-groq"
      ],
      "metadata": {
        "id": "XQtB4HmPzXNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2msOttR0OaE",
        "outputId": "21bf334c-10f2-43f9-ed5f-0190566c6c4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_transformations = Groq(model=\"qwen-2.5-32b\", api_key=os.environ[\"GROQ_API_KEY\"])"
      ],
      "metadata": {
        "id": "z85b4STP7eCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# other transformations\n",
        "\n",
        "from llama_index.core.extractors import (\n",
        "    TitleExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "text_splitter = SentenceSplitter(\n",
        "    separator=\" \", chunk_size=1024, chunk_overlap=128\n",
        ")\n",
        "title_extractor = TitleExtractor(llm=llm_transformations, nodes=5)\n",
        "qa_extractor = QuestionsAnsweredExtractor(llm=llm_transformations, questions=3)\n",
        "\n",
        "\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        text_splitter,\n",
        "        title_extractor,\n",
        "        qa_extractor\n",
        "    ]\n",
        ")\n",
        "\n",
        "nodes = pipeline.run(\n",
        "    documents=docs,\n",
        "    in_place=True,\n",
        "    show_progress=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "50090a1a905847e4a32e1ee94b387005",
            "024e8240fff246c6af50b372b175d481",
            "7ca34bb8a6174036b8150363a68ba733",
            "aaf7609e5f3044fcab76e90fad6e1dbe",
            "6f7fac2c8b764e46a25393d29843d807",
            "c02e809e3725456b85d46cbddee0956c",
            "ffac10754aa945ffa983c5e89eb45337",
            "bb823eb791964f2ca72274df6c16d909",
            "cad42c49057447efbd09d5635048d65c",
            "d323d3db33e6400b97604169d9bd3702",
            "43a392b2d250478db39758426e57ba7e"
          ]
        },
        "collapsed": true,
        "id": "Xtz9ymYyy0T3",
        "outputId": "37b69a5c-181e-4b38-ad27-c0213b18ae59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/19 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50090a1a905847e4a32e1ee94b387005"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  5.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  5.32it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  5.65it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00,  6.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00,  3.94it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00,  4.82it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.32it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  5.13it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.87it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.89it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.19it/s]\n",
            "100%|██████████| 30/30 [00:16<00:00,  1.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, Llamaindex uses OpenAI's embedding models. But you can choose to load a free model from HuggingFace too (but it it will be slower)."
      ],
      "metadata": {
        "id": "bq9Bl6sk28F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHmUuMZ79Qzo",
        "outputId": "bbccab6a-d2df-4bc7-e92e-44a122857a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "# pprint.pprint(nodes[0].__dict__)\n",
        "\n",
        "print(nodes[0].get_content(metadata_mode=MetadataMode.LLM))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AWBHMPxy9SzK",
        "outputId": "1978bc16-8973-4cdc-d198-23ef1ff2ae83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Excerpt from document]\n",
            "page_label: 1\n",
            "file_path: /content/data/2502.09838v2.pdf\n",
            "document_title: Title: HealthGPT: A Comprehensive Medical Vision-Language Model for Multi-Modal Comprehension and Generation in Healthcare Applications\n",
            "\n",
            "This title effectively encapsulates the essence of the document, suggesting that it discusses a sophisticated AI model named HealthGPT, which is designed to handle both visual and textual data in the context of healthcare applications.\n",
            "questions_this_excerpt_can_answer: Based on the provided context, here are three specific questions that this document can answer, which are unlikely to be found elsewhere:\n",
            "\n",
            "1. **What are the specific anatomical locations identified by HealthGPT as showing pulmonary edema, hazy opacity, or mediastinal displacement in a given medical image?**\n",
            "   - This question is specific to the capabilities of HealthGPT and the particular medical images it has analyzed, making the answer unique to this document.\n",
            "\n",
            "2. **How does HealthGPT interpret the purpose of different membrane treatments used in a specific study, and what is the significance of this interpretation in the context of medical research?**\n",
            "   - This question delves into the detailed analysis and interpretation capabilities of HealthGPT, focusing on a specific aspect of medical research that is likely unique to this document.\n",
            "\n",
            "3. **What is the process and outcome of transforming a CT image into an MRI image using HealthGPT, and how does this transformation compare to the original CT image in terms of diagnostic information?**\n",
            "   - This question addresses the specific generation capabilities of HealthGPT, particularly in the context of medical imaging, and the comparison between the transformed and original images, which is likely unique to this document.\n",
            "\n",
            "These questions leverage the detailed and specific information provided by HealthGPT as described in the document, making the answers unique and unlikely to be found elsewhere.\n",
            "Excerpt:\n",
            "-----\n",
            "Metadata:\n",
            "\n",
            "---\n",
            "Content:\n",
            "HealthGPT: A Medical Large Vision-Language Model for Unifying\n",
            "Comprehension and Generation via Heterogeneous Knowledge Adaptation\n",
            "Tianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\n",
            "Mengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\n",
            "1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\n",
            "4The Hong Kong University of Science and Technology,5National University of Singapore\n",
            "Project Page\n",
            " Code\n",
            "1. X-Ray \n",
            "Comprehension\n",
            "2. CT\n",
            "Comprehension\n",
            "3. MRI\n",
            "Comprehension\n",
            "Comp. Perf.\n",
            "7 Medical Multi-Modal Comprehension Tasks\n",
            "Gen. \n",
            "Performance\n",
            "5 Medical Multi-Modal Generation Tasks\n",
            "List all anatomical locations showing \n",
            "pulmonary edema, hazy opacity, or \n",
            "mediastinal displacement.\n",
            "Left hilar structures, left lung,\n",
            "right hilar structures, right lung.\n",
            "Which abdominal organ shows any \n",
            "indication of a lesion or abnormality \n",
            "in the CT image?\n",
            "No abdominal organs show\n",
            "any clear indications of lesions\n",
            "or abnormalities.\n",
            "Could you explain what this mass\n",
            "in the MRI means for my health? \n",
            "Is it very serious?\n",
            "Certainly, the MRI shows a\n",
            "defined mass in your left nasal\n",
            "cavity. There is no sign of the ...\n",
            "4. Microsopy Comprehension\n",
            " What is the purpose of the different \n",
            "membrane treatments used in this \n",
            "study?\n",
            "The purpose of the different\n",
            "membrane treatments used…\n",
            "5. OCT Comprehension\n",
            " What is the purpose of comparing \n",
            "the OCT structure image and OCTA \n",
            "image with H&E histology?\n",
            "To confirm the histological position\n",
            "of the obtained OCT brain images.\n",
            "6. Fundus\n",
            "Comprehension\n",
            " What specific findings or pathological \n",
            "changes can be observed in this \n",
            "fundus image?\n",
            "The fundus image appears normal with\n",
            "no noticeable signs of pathology…\n",
            "7. Ultrasound\n",
            "Comprehension\n",
            " What type of imaging technique \n",
            "is used in this image?\n",
            "The image is a sagittal gray-\n",
            "scale ultrasonographic…\n",
            "1. CT2MRI\n",
            "Generation\n",
            "I need a version of this CT representation \n",
            "in MRI.\n",
            "The image has\n",
            "been transformed\n",
            "into MRI.\n",
            "2. MRI2CT\n",
            "Generation\n",
            "Transform the MRI display into a \n",
            "CT image.\n",
            "Here is the CT\n",
            "version of the\n",
            "MRI image.\n",
            "3. Image Reconstruction\n",
            "Reconstruct the following \n",
            "medical images.\n",
            "Here is the reconstructed\n",
            "medical image you need.\n",
            "4. Super Resolution\n",
            "Could you improve the quality\n",
            "of this MRI image?\n",
            "Here is the image with\n",
            "improved resolution.\n",
            "5. Report-to-CXR\n",
            "The X-ray shows no \n",
            "pleural effusion or \n",
            "pneumothorax.\n",
            "Here is the\n",
            "chest X-ray\n",
            "image for\n",
            "you.\n",
            "Gen. Perf.\n",
            "Figure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\n",
            "unified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\n",
            "plex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\n",
            "Abstract\n",
            "We present HealthGPT, a powerful Medical Large Vision-\n",
            "Language Model (Med-LVLM) that integrates medical vi-\n",
            "sual comprehension and generation capabilities within a uni-\n",
            "fied autoregressive paradigm. Our bootstrapping philosophy\n",
            "is to progressively adapt heterogeneous comprehension and\n",
            "generation knowledge to pre-trained large language mod-\n",
            "els (LLMs). This is achieved through a novel heterogeneous\n",
            "low-rank adaptation (H-LoRA) technique, which is com-\n",
            "plemented by a tailored hierarchical visual perception ap-\n",
            "proach and a three-stage learning strategy. To effectively\n",
            "learn the HealthGPT, we devise a comprehensive medi-\n",
            "cal domain-specific comprehension and generation dataset\n",
            "called VL-Health. Experimental results demonstrate ex-\n",
            "ceptional performance and scalability of HealthGPT in\n",
            "medical visual unified tasks. Our project can be accessed at\n",
            "https://github.com/DCDmllm/HealthGPT.\n",
            "1 Introduction\n",
            "Large Vision-Language Models (LVLMs) (Liu et al. 2023;\n",
            "OpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\n",
            "demonstrated outstanding open-world visual comprehension\n",
            "and reasoning abilities through language-based interactive\n",
            "dialogue over the past years, simultaneously opening up\n",
            "new opportunities for applications in specialized domains.\n",
            "1\n",
            "arXiv:2502.09838v2  [cs.CV]  17 Feb 2025\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index"
      ],
      "metadata": {
        "id": "rQMILxH8_VJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "id": "Q492Kpcaj8GR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfaa9936-8522-4326-a29c-93b0978207d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings\n",
        "\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "hf_embeddings = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "test_embed = hf_embeddings.get_text_embedding(\"Hello world\")\n",
        "print(test_embed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511,
          "referenced_widgets": [
            "6487c86402404a55b65c0c2300d23e90",
            "4f0cc711125d47639ccb5097275d9ddb",
            "90b0e9f87f844241a2bd71ba70649d29",
            "91c9451d22e14c0d8c850cbce10ca4f8",
            "2048854a89c54fb7a4ffff3c2088c93c",
            "cb47f2adc1e3408ea3ea20838110fe7d",
            "530ba97fa1fc4caf9c28a0edeb81fbd6",
            "bbecb7a0f9254d7f950d87e0fb9f7213",
            "32dce4f77ede4c3a8bf6d7c48edac0ad",
            "bb032fab0d8c41809fa277a2d1459ac2",
            "fa123fd9fba34c279c7a612b3cfad602",
            "585f8c0f87fe47d3a6eb3c750d6567ec",
            "0effdaee342b4871abb50088f14c6a31",
            "8c1feecd99774a0eb65e9f009fc8e907",
            "097979c786f3403280c673e880d7040a",
            "585dca54f17f43468b2899f8bf53cbd7",
            "fd769a014cd647348fd998fd01de1456",
            "edb9877a50434060864202fcf91862e4",
            "bf84a30624be445bb594d08415964411",
            "87a8a98697c74d8babe16deb6861e8be",
            "63a251739ae44a94b43ca7120f9ca506",
            "3def064ddadf4d96834426d328775139",
            "54c144fda9b44182ae52fff76c703f32",
            "7919b7d33e8346269fa9ca36b4027ed4",
            "f99f756949984059af5a56867073cdef",
            "dcfc01aba66c46cb9aa7a805485c2676",
            "a3556f7158084884a6c59b33bdf750b7",
            "87ad42306bf94ccc89e4ad052a3d6d57",
            "f14b4da1bdc34b87be985abf78530f7a",
            "a8b05376e7f94bffaa313dd526fef46d",
            "7e6c754afe4c4b658b114ac04b659d06",
            "a72fd6ce2d924d12911a812d8f845eef",
            "3d260ecd240544aa94fd90eded12c660",
            "52907b61e1b041db88a74c8a39d88ca5",
            "42827a5c847c4f60a002b27bcb0c82a9",
            "ee37016a0afa4dafb58563c39a5c5dc2",
            "78812a7bfdfb431489f9f73389932cb6",
            "d4895c0aa7e34d92bc73e640411b8359",
            "a0b578c213df467b981572be91af1eac",
            "525e7fed7c7341fe92527610b10e6257",
            "2c14adf4d49848bc8ac702fc6c86e9a5",
            "053e521768ee4c6cbbc7df1b7d6c35d3",
            "e2d1b57fa636448e9b066e3341ba4958",
            "d528bb4242784a019d66ccd22744e7e1",
            "b0b89fa2b3f040e481493959808f63a4",
            "9daaa444e93c4dda911a79bf6158e401",
            "102bfa341d0041a09aace6a9a1ad6927",
            "94af3670682749a5955fb63eab6b0a80",
            "19e3717c839941129c847079fb85587a",
            "f333896e5f7e404b8d2090f2d1896857",
            "1c6ea5761541414ba2648913c9f68591",
            "04f40645b35740449376e1f5cde65655",
            "38b1bc8a8fa445ff8d38aeff74007294",
            "0540b71ea39d41278b3e19e2b2479f2a",
            "d2bd5c77415e40f39673a8d713916b25",
            "cc3292b9c7d74e8588e3dfb0105d2695",
            "c0e1958aa62448a99d464622743c381c",
            "aa9ef684b2b444b9be45eeeacff70e8f",
            "78fb05bf5a0b4ca3aaaa5b06da0659b7",
            "2643a589bed14e0998b7dabef4f5ed94",
            "b09473fa8eee42338ca04d37215be237",
            "f1e5880d8e204ea0b24bcabcb12fca64",
            "8f032fdb793f46749dce7190fb32afe9",
            "f56f29eaa33d4270b88a40d8fae5262d",
            "862455d1cd7f486b8e5d63471022cd5b",
            "1e3d0cca925b4e0f9bc7cc6e267713dd",
            "af14f159fee14c63886c824e9e13d9de",
            "b66ab1a9d5424b02a333062249287183",
            "c3e41cd749564462add3634d87b80101",
            "e26f6f229c0c4bb6be571a989b5c925a",
            "33869482cc494ba6ad61ab8acef98fa3",
            "4b4f2b94cf7a449f87d7e89ef022ea76",
            "ed2b6611e381425eaf66ee91435c5b4a",
            "3ab82be85e084df89f835bd6c878c496",
            "f9caae9bdc41400982d4d9db05990d7c",
            "02c9bc1b82254197b13c480a8b501b1c",
            "94d6dfcc9c6b4837ac53fc26c8ba3501",
            "0bac2207e58b451886b4392d2df2d605",
            "fda8960bbcb74542b42f9f6a5cdce908",
            "d46ae126e36847acb2a5c59c9e63ac2b",
            "8315ec95bf0143a6a6d9c2d7f1c752c4",
            "3ed4884f50594ac5b218b3def9154996",
            "331d0b5e1964461aa7cf518d53324230",
            "083b917f31364466a1ad859cd42b098a",
            "1a2c13d0b9fd4425bce6bbc7c589f221",
            "f072c6685f8046fc8df3fea11b471b75",
            "684a7baac1904888aac73f5c627b1193",
            "a48a98e17ce64b2681b542b1740de270",
            "b3d69e5a99114c618b6a124e41c1ad0f",
            "3483b19492e447b9b64dbc7a07b5c58d",
            "5ca994e4684d4e32b0548f3d402e0679",
            "d21de1bb5a584b1d83aeac4af6b3d6a4",
            "0cca6ca326434305919d1717799f4e7a",
            "d2634d4a8ec649b6af98f0095a69865f",
            "77ad6a3030694bd79778c286826e77fa",
            "ece3bf82fc4f4d18abecd60268808c56",
            "b3bb7903cd344bcab50747840e4e3043",
            "5282e8fd1a414f8eb588dc87163d716e",
            "6cf4367a56754124b4963d51f8cb4159",
            "de522a701f104625a2d7f55cf40c473e",
            "f3c8d1aa7fa94e9ebd9b45167bce063a",
            "84d26ce841754159a704ac2c2c5e5160",
            "6fb2fd40f99d4f8ea02107a6216de139",
            "3776261b4cdb4ffbbf68d404f8bc0ff8",
            "2c5d1e19a8a14bc4bbb833ecc558c6f7",
            "300cec9a848b4c29bc5320db8bd433d4",
            "699655708fb74eef96c376661a1e3353",
            "be20848f4266409e9c52fe4df00c1cfc",
            "6e6215f9695a4f3c8a0b2ec55b5d5e14",
            "01beea19411141238a63870b26f5879c",
            "0822f2758e44404186c0fe0f65f5b7b3",
            "55c2515d42424875a808f20acf372620",
            "c2106309ac66405f9ba758730451477f",
            "dd70d845be0b4f3888513cec6d48a390",
            "12154920aa1f4358a54cc93ef8d76d6a",
            "cbe1643c6ab047b5b3ff882a19890553",
            "e6edcb2f263741bba36c63b91c281d72",
            "75c3acc0c14642e88d04eb5ff5014954",
            "45d4a79a58554a62a64bb2d8df9dc888",
            "2500cdff07fe40d9a1e47fd7081d4297",
            "a31a75ba72e94b4895d64ce53ad4e6b2"
          ]
        },
        "collapsed": true,
        "id": "gWLHSg0Re_Xr",
        "outputId": "4e0c2143-bfd7-4cff-f14b-b6c2453053f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6487c86402404a55b65c0c2300d23e90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "585f8c0f87fe47d3a6eb3c750d6567ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54c144fda9b44182ae52fff76c703f32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52907b61e1b041db88a74c8a39d88ca5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0b89fa2b3f040e481493959808f63a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc3292b9c7d74e8588e3dfb0105d2695"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af14f159fee14c63886c824e9e13d9de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bac2207e58b451886b4392d2df2d605"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3d69e5a99114c618b6a124e41c1ad0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de522a701f104625a2d7f55cf40c473e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0822f2758e44404186c0fe0f65f5b7b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01519613154232502, -0.022570662200450897, 0.008547082543373108, -0.07417060434818268, 0.003836424555629492, 0.0027135657146573067, -0.03126790374517441, 0.04463401064276695, 0.04405517131090164, -0.007871180772781372, -0.025200815871357918, -0.033366575837135315, 0.014427902176976204, 0.04653819277882576, 0.008555065840482712, -0.01614576019346714, 0.0074058398604393005, -0.019012469798326492, -0.11472626030445099, -0.018157577142119408, 0.12635937333106995, 0.029702914878726006, 0.025281012058258057, -0.03421789035201073, -0.04099970683455467, 0.006617303937673569, 0.010270675644278526, 0.022362256422638893, 0.004436364397406578, -0.1273096203804016, -0.016149284318089485, -0.020380139350891113, 0.04721219092607498, 0.011579902842640877, 0.0681871548295021, 0.007298648823052645, -0.0178530216217041, 0.0407821349799633, -0.01026944350451231, 0.023757033050060272, 0.010602838359773159, -0.02858441136777401, 0.008159711956977844, -0.015180503949522972, 0.030896244570612907, -0.06597989797592163, -0.022196512669324875, 0.05402376502752304, 0.002542274771258235, 0.022452760487794876, -0.0916537269949913, -0.045140340924263, -0.004192084074020386, -0.005621550139039755, -0.005380897782742977, 0.09839349240064621, 0.06052476540207863, 0.007422889117151499, 0.01393856666982174, 0.0026878148782998323, 0.047569356858730316, 0.028636595234274864, -0.1553441435098648, 0.06893699616193771, 0.030248159542679787, -0.017939725890755653, 0.020977139472961426, 0.02140877954661846, 0.014081157743930817, 0.0018777319928631186, 0.0026721421163529158, 0.0038723377510905266, 0.04116382077336311, 0.06589511036872864, -0.006151874084025621, -0.016465358436107635, 0.00819473247975111, -0.0489555299282074, -0.021113477647304535, -0.03084930218756199, -0.04047701135277748, 0.059261009097099304, 0.01816524937748909, -0.04429422318935394, 0.0007034787558950484, -0.02790779247879982, -0.0406387522816658, -0.011253736913204193, -0.024980857968330383, 0.009651425294578075, -0.017492081969976425, -0.027296418324112892, -0.015290331095457077, -0.005397077649831772, -0.041438326239585876, 0.007155370432883501, 0.007107210345566273, 0.00974914152175188, 0.0006180496420711279, 0.34404024481773376, -0.09539385139942169, -0.002025209367275238, 0.02809288166463375, -0.09136316925287247, 0.05958418920636177, 0.024909164756536484, -0.016385646536946297, -0.029118675738573074, -0.008362811990082264, 0.01569257862865925, 0.01284161489456892, -0.06428154557943344, 0.014501035213470459, -0.013740750961005688, 0.0010519773932173848, -0.01968100108206272, 0.050033729523420334, -0.0028084784280508757, 0.09320448338985443, -0.02949247695505619, -0.00804367009550333, 0.030725033953785896, -0.04398002102971077, -0.0042044841684401035, 0.052866268903017044, -0.06449878960847855, 0.05819971114397049, 0.07761349529027939, 0.011616258881986141, 0.06978411227464676, -0.005409490782767534, 0.059834398329257965, -0.026352573186159134, -0.008660356514155865, 0.02755480445921421, -0.014334364794194698, -0.018221525475382805, -0.013942156918346882, 0.03554501757025719, -0.05676673725247383, 0.008173619396984577, -0.07672375440597534, -0.022576918825507164, -0.11284565925598145, 0.0003389387857168913, 0.030381493270397186, -0.07333743572235107, 0.024545667693018913, -0.019619954749941826, -0.024086004123091698, -0.03893819451332092, 0.07869315147399902, 0.004945804364979267, -0.01632915996015072, 0.007779812440276146, 0.05509073659777641, -0.012773742899298668, 0.06841861456632614, 0.007775801233947277, 0.008763236925005913, -0.0018357636872678995, -0.012438087724149227, -0.013271537609398365, 0.006665404886007309, -0.01779901050031185, -0.12814681231975555, 0.009992911480367184, 0.019434839487075806, -0.007243268191814423, 0.000852978671900928, 0.0032817975152283907, 0.016555409878492355, -0.039597220718860626, 0.02890428900718689, 0.10964838415384293, 0.007512431126087904, -0.004082218278199434, 0.044571228325366974, -0.047251660376787186, 0.0251011922955513, 0.0600975900888443, -0.05091461166739464, -0.041688308119773865, 0.019089002162218094, 0.02827546000480652, -0.025334255769848824, -0.020802533254027367, -0.030481716617941856, 0.06234195828437805, 0.06707876920700073, -0.023084701970219612, 0.010649411007761955, -0.03191754221916199, -0.034244805574417114, -0.08421716094017029, 0.003371339989826083, 0.033969711512327194, -0.08108051866292953, 0.013465003110468388, -0.021524492651224136, 0.14621295034885406, 0.05304113030433655, 0.004024855326861143, 0.028765760362148285, 0.00055360805708915, 0.004209362436085939, 0.040644172579050064, 0.006172500550746918, 0.0448748841881752, 0.013396856375038624, -0.02427598275244236, -0.015174142085015774, 0.07311952859163284, -0.0065604206174612045, 0.021938825026154518, -0.0429537259042263, -0.009960740804672241, 0.0746108815073967, 0.023877663537859917, 0.04712563380599022, -0.03976590186357498, 0.01077465433627367, -0.022150542587041855, -0.2623773217201233, 0.018034636974334717, 0.008216139860451221, -0.003416565479710698, -0.03475777804851532, 0.022967888042330742, 0.03806747496128082, -0.05160953104496002, 0.10182670503854752, -0.009048509411513805, 0.08706742525100708, -0.059638116508722305, -0.008330157957971096, -0.03651214763522148, 0.017570778727531433, 0.023195451125502586, -0.014176588505506516, 0.01601136475801468, -0.010097771883010864, -0.02271001972258091, 0.02862256020307541, 0.02296573296189308, 0.04340479522943497, -0.04762280732393265, 0.04443009942770004, -0.05967699736356735, 0.14656038582324982, 0.08372188359498978, -0.020243028178811073, 0.024193381890654564, 0.03635025769472122, -0.02795586921274662, -0.009284982457756996, -0.11986202746629715, -0.025538844987750053, 0.07363226264715195, -0.03464934602379799, -0.06727727502584457, -0.09658315777778625, -0.0222835261374712, -0.01242988184094429, 0.013772562146186829, -0.040937796235084534, -0.004330980125814676, -0.02414258010685444, -0.07481462508440018, -0.05261693522334099, 0.009800228290259838, -0.0520673543214798, -0.012455124408006668, -0.011691379360854626, 0.0223501268774271, 0.05714461952447891, 0.06000139191746712, 0.019016524776816368, -0.04594933241605759, 0.0016673022182658315, -0.0006489161751233041, -0.0114967729896307, 0.03237845376133919, -0.014662795700132847, -0.022220883518457413, 0.015859412029385567, -0.03663552179932594, 0.011533604003489017, 0.03506964445114136, -0.061063606292009354, -0.024883558973670006, 0.049819473177194595, -0.017432786524295807, -0.018123142421245575, -0.03571717441082001, 0.02123081125319004, -0.016480140388011932, 0.03630690649151802, 0.014197791926562786, -0.0045011029578745365, -0.023299407213926315, -0.03982175141572952, -0.02817395329475403, -0.005503759253770113, 0.011411167681217194, 0.058362334966659546, 0.01423725951462984, 0.03271001577377319, 0.05409242957830429, 0.06469085812568665, 0.007722494658082724, 0.035451725125312805, -0.01605343073606491, -0.012950776144862175, 0.041223783046007156, -0.005376933608204126, -0.06980964541435242, 0.01131397020071745, 0.016101401299238205, -0.29501014947891235, 0.02777673862874508, -0.0029649606440216303, 0.021393120288848877, 0.00406600208953023, 0.021163562312722206, 0.041121724992990494, -0.00041538773803040385, -0.05732302740216255, 0.022310476750135422, -0.07742249220609665, 0.020380808040499687, 0.01625850610435009, -0.06689383089542389, 0.000815753941424191, 0.020208537578582764, -0.0024549805093556643, -0.011008543893694878, 0.01705600507557392, -0.019496070221066475, 0.00204800465144217, 0.022157587110996246, 0.22987931966781616, -0.02301955223083496, 0.0567016638815403, 0.03906967490911484, -0.009256253018975258, 0.004575187340378761, 0.05478924885392189, 0.019259154796600342, -0.09813746809959412, -0.00015211237769108266, 0.03151262551546097, -0.015674259513616562, 0.035405755043029785, 0.01093531958758831, -0.06798958033323288, -0.028932519257068634, 0.02402884140610695, -0.05310331657528877, -0.02500537410378456, 0.022354330867528915, -0.046019118279218674, 0.07040763646364212, 0.03455902636051178, -0.07733150571584702, -0.013528752140700817, -0.04894503951072693, -0.003992392215877771, 0.03734888881444931, -0.0281567070633173, -0.07967019081115723, 0.005711013916879892, 0.032059043645858765, -0.030483229085803032, 0.015031400136649609, 0.014759581536054611, -0.009081630036234856, 0.01614023745059967, -0.063435398042202, 0.021307507529854774, -0.006121619138866663, 0.04932292178273201, 0.02275387942790985, 0.026068123057484627]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create index\n",
        "\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex(nodes, embed_model=hf_embeddings)"
      ],
      "metadata": {
        "id": "1Ffev4Gj9tA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query"
      ],
      "metadata": {
        "id": "rbXqTcgH-LPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_querying = Groq(model=\"llama-3.3-70b-versatile\", api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llm_querying)\n",
        "response = query_engine.query(\n",
        "    \"what does this model do?\"\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLuIYx5Q-K2T",
        "outputId": "2064c3be-b7cb-425a-97cc-461877933d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model, specifically referred to in the context as various versions and types such as HealthGPT, Med-Flamingo, LLaV A-Med, and others, is designed for multimodal learning tasks. These tasks include visual comprehension, where the model processes and understands visual data, and generation tasks, where the model generates images or text based on given inputs. The models are also compared in terms of their performance in comprehension tasks, indicating their ability to understand and process complex information from different modalities, such as vision and language. Additionally, some models like Moelora and Lumina-MGPT are focused on specific aspects such as contrastive learning guided mixture of experts for parameter-efficient fine-tuning and photorealistic text-to-image generation, respectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.__dict__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DXXv87vl-2PQ",
        "outputId": "826651fc-1969-4bea-bbe0-0e4d5e51b7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': 'The model, specifically referred to in the context as various versions and types such as HealthGPT, Med-Flamingo, LLaV A-Med, and others, is designed for multimodal learning tasks. These tasks include visual comprehension, where the model processes and understands visual data, and generation tasks, where the model generates images or text based on given inputs. The models are also compared in terms of their performance in comprehension tasks, indicating their ability to understand and process complex information from different modalities, such as vision and language. Additionally, some models like Moelora and Lumina-MGPT are focused on specific aspects such as contrastive learning guided mixture of experts for parameter-efficient fine-tuning and photorealistic text-to-image generation, respectively.',\n",
              " 'source_nodes': [NodeWithScore(node=TextNode(id_='934d50af-1ceb-457b-8563-07dbef58b550', embedding=None, metadata={'page_label': '10', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23', 'document_title': 'Title: Advances in Multimodal Learning: Integrating Vision, Language, and Medical Knowledge with Transformer-Based Attention Mechanisms\\n\\nThis comprehensive title encapsulates the focus on multimodal learning advancements, the integration of various types of data (vision, language, medical), and the use of transformer-based models with attention mechanisms, reflecting a broad yet specific scope as suggested by the candidate titles.', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are three specific questions that this document can answer, which are unlikely to be found elsewhere:\\n\\n1. **What are the key contributions of the \"Lumina-MGPT\" model as described in the arXiv preprint by Liu et al. (2024a), and how does it enhance photorealistic text-to-image generation in a multimodal context?**\\n   - This question targets a specific contribution of a particular paper, focusing on the advancements in multimodal generative pretraining and photorealistic text-to-image generation. The document provides a direct reference to this work, making it a unique source for this information.\\n\\n2. **How does the \"VMamba: Visual State Space Model\" (arXiv preprint by Liu et al., 2024e) integrate visual state space modeling, and what are its implications for multimodal learning?**\\n   - This question delves into the specifics of the VMamba model, which is mentioned in the document. It asks about the integration of visual state space modeling and its implications, which would be detailed in the referenced paper and not likely to be summarized elsewhere.\\n\\n3. **What is the methodology behind \"Moelora: Contrastive Learning Guided Mixture of Experts\" (arXiv preprint by Luo et al., 2024a), and how does it improve parameter-efficient fine-tuning for large language models?**\\n   - This question focuses on the methodology and improvements of the Moelora model, as described in the referenced paper. The document provides a citation to this work, making it a unique source for detailed information on this specific approach to parameter-efficient fine-tuning.\\n\\nThese questions are designed to extract detailed and specific information from the document that is unlikely to be replicated in other sources, leveraging the unique references and citations provided in the context.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='61b79e41-c04e-457c-ae45-4bc131935512', node_type='4', metadata={'page_label': '10', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, hash='f05dfd2ecaea03daf44759c8b6720351c7ccedb1f67649b4c9909c22efe5c685'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bcb2f03d-02b6-44e4-b6e5-a90d491c3979', node_type='1', metadata={}, hash='1bd076d78907fbd6cc3578ab006bfa18f089d234112db38b628fd42bb0f54f7e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Metadata:\\n\\n---\\nContent:\\nrank adaptation with expert collaboration and competition.\\narXiv preprint arXiv:2408.09856.\\nLiu, B.; Zhan, L.-M.; Xu, L.; Ma, L.; Yang, Y .; and Wu,\\nX.-M. 2021. Slake: A semantically-labeled knowledge-\\nenhanced dataset for medical visual question answering. In\\n2021 IEEE 18th International Symposium on Biomedical\\nImaging (ISBI), 1650–1654. IEEE.\\nLiu, D.; Zhao, S.; Zhuo, L.; Lin, W.; Qiao, Y .; Li, H.; and\\nGao, P. 2024a. Lumina-mgpt: Illuminate flexible photore-\\nalistic text-to-image generation with multimodal generative\\npretraining. arXiv preprint arXiv:2408.02657.\\nLiu, H.; Li, C.; Li, Y .; and Lee, Y . J. 2024b. Improved\\nbaselines with visual instruction tuning. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 26296–26306.\\nLiu, H.; Li, C.; Li, Y .; Li, B.; Zhang, Y .; Shen, S.; and Lee,\\nY . J. 2024c. LLaV A-NeXT: Improved reasoning, OCR, and\\nworld knowledge. https://llava-vl.github.io/blog/2024-01-\\n30-llava-next/.\\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual Instruc-\\ntion Tuning. In NeurIPS.\\nLiu, Q.; Wu, X.; Zhao, X.; Zhu, Y .; Xu, D.; Tian, F.; and\\nZheng, Y . 2024d. When moe meets llms: Parameter efficient\\nfine-tuning for multi-task medical applications. In Proceed-\\nings of the 47th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval, 1104–\\n1114.\\nLiu, Y .; Tian, Y .; Zhao, Y .; Yu, H.; Xie, L.; Wang, Y .; Ye,\\nQ.; and Liu, Y . 2024e. VMamba: Visual State Space Model.\\narXiv preprint arXiv:2401.10166.\\nLu, J.; Clark, C.; Lee, S.; Zhang, Z.; Khosla, S.; Marten, R.;\\nHoiem, D.; and Kembhavi, A. 2024. Unified-IO 2: Scaling\\nAutoregressive Multimodal Models with Vision Language\\nAudio and Action. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , 26439–\\n26455.\\nLu, J.; Clark, C.; Zellers, R.; Mottaghi, R.; and Kembhavi,\\nA. 2022. Unified-io: A unified model for vision, language,\\nand multi-modal tasks. In The Eleventh International Con-\\nference on Learning Representations.\\nLuo, T.; Lei, J.; Lei, F.; Liu, W.; He, S.; Zhao, J.; and Liu,\\nK. 2024a. Moelora: Contrastive learning guided mixture of\\nexperts on parameter-efficient fine-tuning for large language\\nmodels. arXiv preprint arXiv:2402.12851.\\nLuo, Y .; Zhang, J.; Fan, S.; Yang, K.; Hong, M.; Wu, Y .;\\nQiao, M.; and Nie, Z. 2024b. Biomedgpt: An open multi-\\nmodal large language model for biomedicine. IEEE Journal\\nof Biomedical and Health Informatics.\\nMasoudnia, S.; and Ebrahimpour, R. 2014. Mixture of ex-\\nperts: a literature survey. Artificial Intelligence Review, 42:\\n275–293.\\nMoor, M.; Huang, Q.; Wu, S.; Yasunaga, M.; Dalmia, Y .;\\nLeskovec, J.; Zakka, C.; Reis, E. P.; and Rajpurkar, P. 2023.\\nMed-flamingo: a multimodal medical few-shot learner. In\\nMachine Learning for Health (ML4H), 353–367. PMLR.\\nNath, V .', mimetype='text/plain', start_char_idx=0, end_char_idx=2833, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.6922238357856348),\n",
              "  NodeWithScore(node=TextNode(id_='28b09b6b-7b41-4295-af1e-becc30d1cfd7', embedding=None, metadata={'page_label': '6', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23', 'document_title': 'Title: Comprehensive Comparative Analysis of HealthGPT and Other Large Vision-Language Models (LVLMs) in Medical Visual Comprehension, Multimodal Conversion, and Image Generation Tasks: Evaluation on VL-Health Dataset (2023-2024)', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt, here are three specific questions that this context can answer, which are unlikely to be found elsewhere:\\n\\n1. **What are the specific settings and configurations used for the H-LoRA model in the visual comprehension and generation tasks?**\\n   - This context provides detailed information about the rank settings (16 and 64) and the number of experts (four) used for the H-LoRA model, which is specific to this study and unlikely to be found in other documents.\\n\\n2. **Which versions of VQGAN are utilized in the image indexing and upsampling module for the experiments conducted in this study?**\\n   - The excerpt mentions the use of the f8-8192 version of VQGAN for the image indexing and upsampling module, which is a specific detail that is unlikely to be found in other studies or documents.\\n\\n3. **What are the specific models compared against HealthGPT in the comprehension tasks, and which categories do they belong to?**\\n   - The context lists a detailed comparison of HealthGPT with several other models, including both medical-specific LVLMs (e.g., Med-Flamingo, LLaV A-Med, HuatuoGPT-Vision) and recent open-world LVLMs (e.g., BLIP-2, LLaV A-v1.5, InstructBLIP, Yi-VL, InternVL2). This specific list of models and their categorization is unique to this study and unlikely to be found elsewhere.\\n\\nThese questions are designed to leverage the unique and detailed information provided in the excerpt, making them unlikely to be answered by other sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0b816d51-7595-4e34-9554-b3965c572618', node_type='4', metadata={'page_label': '6', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, hash='ef90f4fad6de8c002a87ee548c86f1c01ae78d0ca1ace77cc29e6824fcd3e939'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9a51904a-2941-4b2a-afc1-f292969b8f91', node_type='1', metadata={'page_label': '6', 'file_name': '2502.09838v2.pdf', 'file_path': '/content/data/2502.09838v2.pdf', 'file_type': 'application/pdf', 'file_size': 8787043, 'creation_date': '2025-02-23', 'last_modified_date': '2025-02-23'}, hash='977111ec25ca7c0a3c761b3a6881e303c421d6f7c4eb6daf7fc2880004c6d981')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2024) and phi-4 (Abdin et al. 2024) as the\\nbase model. For visual comprehension and generation tasks,\\nwe set the rank of H-LoRA to 16 and 64, with four experts.\\nAdditionally, we use the f8-8192 version of VQGAN as the\\nimage indexing and upsampling module.\\n5.2 Main Experiments\\nComprehension. We compare HealthGPT with several\\nexisting models, including medical-specific LVLMs (e.g.,\\nMed-Flamingo (Moor et al. 2023), LLaV A-Med (Li et al.\\n2024b), HuatuoGPT-Vision (Chen et al. 2024a)) as well\\nas recent open-world LVLMs (e.g., BLIP-2 (Li et al.\\n2023b), LLaV A-v1.5 (Liu et al. 2024b), InstructBLIP (Dai\\net al. 2023), Yi-VL (Young et al. 2024), InternVL2 (Chen\\n6', mimetype='text/plain', start_char_idx=3658, end_char_idx=4318, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.6919979202375961)],\n",
              " 'metadata': {'934d50af-1ceb-457b-8563-07dbef58b550': {'page_label': '10',\n",
              "   'file_name': '2502.09838v2.pdf',\n",
              "   'file_path': '/content/data/2502.09838v2.pdf',\n",
              "   'file_type': 'application/pdf',\n",
              "   'file_size': 8787043,\n",
              "   'creation_date': '2025-02-23',\n",
              "   'last_modified_date': '2025-02-23',\n",
              "   'document_title': 'Title: Advances in Multimodal Learning: Integrating Vision, Language, and Medical Knowledge with Transformer-Based Attention Mechanisms\\n\\nThis comprehensive title encapsulates the focus on multimodal learning advancements, the integration of various types of data (vision, language, medical), and the use of transformer-based models with attention mechanisms, reflecting a broad yet specific scope as suggested by the candidate titles.',\n",
              "   'questions_this_excerpt_can_answer': 'Based on the provided context, here are three specific questions that this document can answer, which are unlikely to be found elsewhere:\\n\\n1. **What are the key contributions of the \"Lumina-MGPT\" model as described in the arXiv preprint by Liu et al. (2024a), and how does it enhance photorealistic text-to-image generation in a multimodal context?**\\n   - This question targets a specific contribution of a particular paper, focusing on the advancements in multimodal generative pretraining and photorealistic text-to-image generation. The document provides a direct reference to this work, making it a unique source for this information.\\n\\n2. **How does the \"VMamba: Visual State Space Model\" (arXiv preprint by Liu et al., 2024e) integrate visual state space modeling, and what are its implications for multimodal learning?**\\n   - This question delves into the specifics of the VMamba model, which is mentioned in the document. It asks about the integration of visual state space modeling and its implications, which would be detailed in the referenced paper and not likely to be summarized elsewhere.\\n\\n3. **What is the methodology behind \"Moelora: Contrastive Learning Guided Mixture of Experts\" (arXiv preprint by Luo et al., 2024a), and how does it improve parameter-efficient fine-tuning for large language models?**\\n   - This question focuses on the methodology and improvements of the Moelora model, as described in the referenced paper. The document provides a citation to this work, making it a unique source for detailed information on this specific approach to parameter-efficient fine-tuning.\\n\\nThese questions are designed to extract detailed and specific information from the document that is unlikely to be replicated in other sources, leveraging the unique references and citations provided in the context.'},\n",
              "  '28b09b6b-7b41-4295-af1e-becc30d1cfd7': {'page_label': '6',\n",
              "   'file_name': '2502.09838v2.pdf',\n",
              "   'file_path': '/content/data/2502.09838v2.pdf',\n",
              "   'file_type': 'application/pdf',\n",
              "   'file_size': 8787043,\n",
              "   'creation_date': '2025-02-23',\n",
              "   'last_modified_date': '2025-02-23',\n",
              "   'document_title': 'Title: Comprehensive Comparative Analysis of HealthGPT and Other Large Vision-Language Models (LVLMs) in Medical Visual Comprehension, Multimodal Conversion, and Image Generation Tasks: Evaluation on VL-Health Dataset (2023-2024)',\n",
              "   'questions_this_excerpt_can_answer': 'Based on the provided excerpt, here are three specific questions that this context can answer, which are unlikely to be found elsewhere:\\n\\n1. **What are the specific settings and configurations used for the H-LoRA model in the visual comprehension and generation tasks?**\\n   - This context provides detailed information about the rank settings (16 and 64) and the number of experts (four) used for the H-LoRA model, which is specific to this study and unlikely to be found in other documents.\\n\\n2. **Which versions of VQGAN are utilized in the image indexing and upsampling module for the experiments conducted in this study?**\\n   - The excerpt mentions the use of the f8-8192 version of VQGAN for the image indexing and upsampling module, which is a specific detail that is unlikely to be found in other studies or documents.\\n\\n3. **What are the specific models compared against HealthGPT in the comprehension tasks, and which categories do they belong to?**\\n   - The context lists a detailed comparison of HealthGPT with several other models, including both medical-specific LVLMs (e.g., Med-Flamingo, LLaV A-Med, HuatuoGPT-Vision) and recent open-world LVLMs (e.g., BLIP-2, LLaV A-v1.5, InstructBLIP, Yi-VL, InternVL2). This specific list of models and their categorization is unique to this study and unlikely to be found elsewhere.\\n\\nThese questions are designed to leverage the unique and detailed information provided in the excerpt, making them unlikely to be answered by other sources.'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store"
      ],
      "metadata": {
        "id": "XLXyCIIZ_crt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index.storage_context.persist(persist_dir=\"./vectors\")"
      ],
      "metadata": {
        "id": "fSvxtaGC_eRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "# rebuild storage context\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"./vectors\")\n",
        "\n",
        "# load index\n",
        "index_from_storage = load_index_from_storage(storage_context, embed_model=hf_embeddings)"
      ],
      "metadata": {
        "id": "0Vq6DhIU_0GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = index_from_storage.as_query_engine(llm=llm_querying)"
      ],
      "metadata": {
        "id": "n6UPLaRvE-5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa.query(\"what does this model do?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8caGYDSyFPbr",
        "outputId": "2d55f94f-8ba1-42fe-c3e4-41ccae078d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model, specifically referred to in the context as various versions and types such as HealthGPT, Med-Flamingo, LLaV A-Med, and others, is designed for multimodal learning tasks. These tasks include visual comprehension, where the model processes and understands visual data, and generation tasks, where the model generates images or text based on given inputs. The models are also compared in terms of their performance in comprehension tasks, indicating their ability to understand and process complex information from different modalities, such as vision and language. Additionally, some models like Moelora and Lumina-MGPT are focused on specific aspects such as contrastive learning guided mixture of experts for parameter-efficient fine-tuning and photorealistic text-to-image generation, respectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Vector Stores"
      ],
      "metadata": {
        "id": "oXSPYn9IFZ_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq chromadb\n",
        "%pip install -Uq llama-index-vector-stores-chroma"
      ],
      "metadata": {
        "id": "DUNlAGCUFcSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e3e6c0-c4f6-4f6f-bce7-d16b53d7a3ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "# initialize client, setting path to save data\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# create collection\n",
        "chroma_collection = db.get_or_create_collection(\"healthGPT\")\n",
        "\n",
        "# assign chroma as the vector_store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# create your index\n",
        "index = VectorStoreIndex(\n",
        "    nodes, storage_context=storage_context, embed_model=hf_embeddings\n",
        ")\n",
        "\n",
        "# You can also load from documents and apply transformations in place\n",
        "# index = VectorStoreIndex.from_documents(\n",
        "#     documents, storage_context=storage_context, transformations=[]\n",
        "# )\n",
        "\n",
        "# Or you can initialize your index from your vector store and then add the nodes\n",
        "# index = VectorStoreIndex.from_vector_store(\n",
        "#     vector_store=vector_store, embed_model=hf_embeddings\n",
        "# )\n",
        "# index.insert_nodes(nodes)\n",
        "\n",
        "\n",
        "# create a query engine and query\n",
        "query_engine = index.as_query_engine(llm=llm_querying)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eo4G5gyGHuBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is this model good at?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR6mFCDaIdAM",
        "outputId": "a7492e86-d848-4149-fc3a-4154c3aee1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This model, specifically HealthGPT-L14, excels across all sub-tasks, achieving optimal or near-optimal results with an average score of 74.4, significantly surpassing other models. It is particularly good at medical visual question answering and image reconstruction tasks, demonstrating stable reconstruction performance even with a small amount of data. Additionally, it performs well in comprehension tasks, often being selected as the best answer by clinicians in human evaluation.\n"
          ]
        }
      ]
    }
  ]
}