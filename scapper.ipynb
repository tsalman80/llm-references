{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Florida License Scraper - Step by Step Guide\n",
    "\n",
    "This notebook walks through the process of scraping license data from the Florida Department of Business and Professional Regulation website: https://www.myfloridalicense.com/wl11.asp?mode=1&SID=&brd=&typ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "First, let's install the necessary Python libraries if you don't already have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install requests beautifulsoup4 pandas lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "\n",
    "Now let's import all the libraries we'll need for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin, parse_qs, urlparse\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set Up Request Headers\n",
    "\n",
    "To mimic a legitimate browser, we'll set up proper request headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create headers to mimic a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Referer': 'https://www.myfloridalicense.com/wl11.asp',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Explore the Initial Page\n",
    "\n",
    "Let's examine the structure of the main search page to understand what options are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up the base URL and parameters\n",
    "base_url = \"https://www.myfloridalicense.com/wl11.asp\"\n",
    "params = {\n",
    "    'mode': '1',\n",
    "    'SID': ''\n",
    "}\n",
    "\n",
    "# Create a session to maintain cookies\n",
    "session = requests.Session()\n",
    "\n",
    "# Make the initial request\n",
    "response = session.get(base_url, params=params, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(f\"Successfully accessed the page: {response.url}\")\n",
    "else:\n",
    "    print(f\"Failed to access the page: {response.status_code}\")\n",
    "    \n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Print the title to confirm we're on the right page\n",
    "print(f\"Page title: {soup.title.text if soup.title else 'No title found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract Available Search Options\n",
    "\n",
    "Let's look at the available boards and license types to understand what we can search for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find all forms on the page\n",
    "forms = soup.find_all('form')\n",
    "boards_dict = {}\n",
    "license_types_dict = {}\n",
    "\n",
    "# Extract the options from each form\n",
    "for form in forms:\n",
    "    selects = form.find_all('select')\n",
    "    \n",
    "    for select in selects:\n",
    "        select_name = select.get('name', '').strip()\n",
    "        print(f\"\\nSelect field: '{select_name}'\")\n",
    "        \n",
    "        options = select.find_all('option')\n",
    "        \n",
    "        for option in options:\n",
    "            value = option.get('value', '').strip()\n",
    "            text = option.text.strip()\n",
    "            \n",
    "            if value and text:\n",
    "                print(f\"  - {text}: {value}\")\n",
    "                \n",
    "                # Store boards and license types in dictionaries\n",
    "                if select_name.lower() == 'brd':\n",
    "                    boards_dict[value] = text\n",
    "                elif select_name.lower() == 'typ':\n",
    "                    license_types_dict[value] = text\n",
    "\n",
    "# Save the options to dataframes\n",
    "boards_df = pd.DataFrame(list(boards_dict.items()), columns=['Code', 'Board Name'])\n",
    "license_types_df = pd.DataFrame(list(license_types_dict.items()), columns=['Code', 'License Type'])\n",
    "\n",
    "print(f\"\\nFound {len(boards_dict)} boards and {len(license_types_dict)} license types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the boards and license types more closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display boards\n",
    "print(\"Available Boards:\")\n",
    "boards_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display license types\n",
    "print(\"Available License Types:\")\n",
    "license_types_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create a Function to Perform a Search\n",
    "\n",
    "Now let's create a function to submit a search with specific board and license type parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def perform_search(session, board=None, license_type=None):\n",
    "    \"\"\"\n",
    "    Performs a search on the Florida license website with the given parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    session : requests.Session\n",
    "        Active session to maintain cookies\n",
    "    board : str, optional\n",
    "        Board code to search for\n",
    "    license_type : str, optional\n",
    "        License type code to search for\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    BeautifulSoup object of the search results page\n",
    "    \"\"\"\n",
    "    # Set up the base URL and parameters\n",
    "    base_url = \"https://www.myfloridalicense.com/wl11.asp\"\n",
    "    params = {\n",
    "        'mode': '1',\n",
    "        'SID': ''\n",
    "    }\n",
    "    \n",
    "    if board:\n",
    "        params['brd'] = board\n",
    "    \n",
    "    if license_type:\n",
    "        params['typ'] = license_type\n",
    "    \n",
    "    # Add a delay to avoid overloading the server\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "    # Make the request\n",
    "    response = session.get(base_url, params=params, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to perform search: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Search performed successfully: {response.url}\")\n",
    "    \n",
    "    # Parse the HTML\n",
    "    return BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Extract Data from Search Results\n",
    "\n",
    "Let's create a function to extract data from the search results page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_search_results(soup):\n",
    "    \"\"\"\n",
    "    Extracts table data from the search results page\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    soup : BeautifulSoup\n",
    "        Parsed HTML of the search results page\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame containing the extracted data\n",
    "    \"\"\"\n",
    "    if not soup:\n",
    "        return None\n",
    "    \n",
    "    # Find all tables on the page\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    if not tables:\n",
    "        print(\"No tables found on the page.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(tables)} tables on the page.\")\n",
    "    \n",
    "    # Find the largest table (likely the results table)\n",
    "    largest_table = max(tables, key=lambda t: len(t.find_all('tr')), default=None)\n",
    "    \n",
    "    if not largest_table:\n",
    "        print(\"No usable data tables found.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract table headers\n",
    "    headers = []\n",
    "    header_rows = largest_table.find_all('tr')\n",
    "    \n",
    "    if header_rows:\n",
    "        header_row = header_rows[0]\n",
    "        headers = [th.text.strip() for th in header_row.find_all(['th', 'td'])]\n",
    "    \n",
    "    if not headers:\n",
    "        print(\"Could not determine table headers.\")\n",
    "        # Create generic headers\n",
    "        sample_row = largest_table.find_all('tr')[1] if len(largest_table.find_all('tr')) > 1 else None\n",
    "        if sample_row:\n",
    "            column_count = len(sample_row.find_all(['td', 'th']))\n",
    "            headers = [f\"Column_{i}\" for i in range(1, column_count + 1)]\n",
    "    \n",
    "    # Extract table data\n",
    "    data = []\n",
    "    \n",
    "    # Skip header row if it exists\n",
    "    rows = largest_table.find_all('tr')[1:] if headers else largest_table.find_all('tr')\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        if cells:\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            \n",
    "            # Also extract links if present\n",
    "            links = []\n",
    "            for cell in cells:\n",
    "                a_tags = cell.find_all('a')\n",
    "                cell_links = [a.get('href') for a in a_tags if a.get('href')]\n",
    "                links.extend(cell_links)\n",
    "            \n",
    "            # Only add rows that have data\n",
    "            if any(cell for cell in row_data):\n",
    "                # Add links to the row data if they exist\n",
    "                if links:\n",
    "                    row_data.append('|'.join(links))\n",
    "                data.append(row_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if data and headers:\n",
    "        # Adjust headers if we added a links column\n",
    "        if data and len(data[0]) > len(headers):\n",
    "            headers.append('Links')\n",
    "        \n",
    "        # Ensure data rows match header length\n",
    "        standardized_data = []\n",
    "        for row in data:\n",
    "            if len(row) < len(headers):\n",
    "                # Pad with empty strings if needed\n",
    "                row = row + [''] * (len(headers) - len(row))\n",
    "            elif len(row) > len(headers):\n",
    "                # Truncate if too long\n",
    "                row = row[:len(headers)]\n",
    "            standardized_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(standardized_data, columns=headers)\n",
    "        print(f\"Successfully extracted {len(df)} records with {len(headers)} columns.\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No data extracted from the table.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the Search Function\n",
    "\n",
    "Let's try searching for a specific board and license type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# For this example, let's search for a specific board\n",
    "# Choose a board from the boards_df we created earlier\n",
    "# For example, let's search for \"Construction Industry Licensing Board\"\n",
    "\n",
    "test_board = \"15\"  # Replace with a valid board code from your boards_df\n",
    "test_license_type = None  # You can set this to a specific license type if desired\n",
    "\n",
    "# Perform the search\n",
    "search_results_soup = perform_search(session, board=test_board, license_type=test_license_type)\n",
    "\n",
    "# Extract the data\n",
    "results_df = extract_search_results(search_results_soup)\n",
    "\n",
    "if results_df is not None and not results_df.empty:\n",
    "    # Display the first few rows\n",
    "    print(\"\\nSample of search results:\")\n",
    "    results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Handle Pagination\n",
    "\n",
    "The search results might span multiple pages. Let's create a function to handle pagination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_next_page_url(soup, base_url):\n",
    "    \"\"\"\n",
    "    Extracts the next page URL from pagination links\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    soup : BeautifulSoup\n",
    "        Parsed HTML of the current page\n",
    "    base_url : str\n",
    "        Base URL for building absolute URLs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str or None: URL of the next page or None if no next page is found\n",
    "    \"\"\"\n",
    "    # Look for pagination links (often at the bottom of the page)\n",
    "    # This may need adjustment based on the actual site structure\n",
    "    pagination_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    next_page_url = None\n",
    "    \n",
    "    # Look for links containing \"Next\" or similar text\n",
    "    for link in pagination_links:\n",
    "        if re.search(r'next|forward|>|Â»', link.text.lower()):\n",
    "            next_page_url = urljoin(base_url, link.get('href'))\n",
    "            return next_page_url\n",
    "    \n",
    "    # If no explicit \"Next\" link, look for numeric pagination\n",
    "    page_numbers = []\n",
    "    current_page = None\n",
    "    \n",
    "    for link in pagination_links:\n",
    "        # Try to find page numbers in the link text\n",
    "        match = re.search(r'\\d+', link.text)\n",
    "        if match:\n",
    "            page_number = int(match.group())\n",
    "            page_numbers.append(page_number)\n",
    "            \n",
    "            # Check if this appears to be the current page (often highlighted or not a link)\n",
    "            if 'current' in link.get('class', []) or not link.get('href'):\n",
    "                current_page = page_number\n",
    "    \n",
    "    # If we identified the current page and there's a higher page number available\n",
    "    if current_page and page_numbers:\n",
    "        next_page_number = current_page + 1\n",
    "        if next_page_number in page_numbers:\n",
    "            for link in pagination_links:\n",
    "                if str(next_page_number) == link.text.strip():\n",
    "                    next_page_url = urljoin(base_url, link.get('href'))\n",
    "                    return next_page_url\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Create a Function to Scrape All Pages\n",
    "\n",
    "Now let's create a function that combines everything to scrape all pages of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def scrape_all_pages(session, board=None, license_type=None, max_pages=10):\n",
    "    \"\"\"\n",
    "    Scrapes all result pages for a given search query\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    session : requests.Session\n",
    "        Active session to maintain cookies\n",
    "    board : str, optional\n",
    "        Board code to search for\n",
    "    license_type : str, optional\n",
    "        License type code to search for\n",
    "    max_pages : int, default=10\n",
    "        Maximum number of pages to scrape\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame containing all scraped data\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.myfloridalicense.com/wl11.asp\"\n",
    "    all_results = []\n",
    "    page_count = 0\n",
    "    \n",
    "    # Get the first page of results\n",
    "    soup = perform_search(session, board, license_type)\n",
    "    \n",
    "    while soup and page_count < max_pages:\n",
    "        page_count += 1\n",
    "        print(f\"Processing page {page_count}...\")\n",
    "        \n",
    "        # Extract data from the current page\n",
    "        df = extract_search_results(soup)\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            all_results.append(df)\n",
    "        \n",
    "        # Find the next page URL\n",
    "        next_page_url = extract_next_page_url(soup, base_url)\n",
    "        \n",
    "        if not next_page_url:\n",
    "            print(\"No more pages found.\")\n",
    "            break\n",
    "        \n",
    "        # Add a delay before the next request\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        \n",
    "        # Request the next page\n",
    "        response = session.get(next_page_url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to access page {page_count + 1}: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Combine all results into a single DataFrame\n",
    "    if all_results:\n",
    "        combined_df = pd.concat(all_results, ignore_index=True)\n",
    "        print(f\"Total records scraped: {len(combined_df)}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Extract License Details\n",
    "\n",
    "Now let's create a function to extract details from individual license pages, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_license_details(session, detail_url):\n",
    "    \"\"\"\n",
    "    Extracts detailed information from a license detail page\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    session : requests.Session\n",
    "        Active session to maintain cookies\n",
    "    detail_url : str\n",
    "        URL to the detail page\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing detailed license information\n",
    "    \"\"\"\n",
    "    # Add a delay to avoid overwhelming the server\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "    # Request the detail page\n",
    "    response = session.get(detail_url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access detail page: {response.status_code}\")\n",
    "        return {}\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract data from the detail page\n",
    "    details = {}\n",
    "    \n",
    "    # Find all tables (details are often in tables)\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            if len(cells) >= 2:\n",
    "                key = cells[0].text.strip().rstrip(':')\n",
    "                value = cells[1].text.strip()\n",
    "                if key and value:\n",
    "                    details[key] = value\n",
    "    \n",
    "    return details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Full Scraping Example\n",
    "\n",
    "Let's put everything together with a full example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Choose a board and license type\n",
    "selected_board = \"15\"  # Construction Industry Licensing Board\n",
    "selected_license_type = None  # All license types\n",
    "\n",
    "# Scrape all pages of results\n",
    "results_df = scrape_all_pages(session, board=selected_board, license_type=selected_license_type, max_pages=3)\n",
    "\n",
    "if results_df is not None and not results_df.empty:\n",
    "    # Display the first few rows\n",
    "    print(\"\\nSample of scraped data:\")\n",
    "    display(results_df.head())\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_filename = f\"florida_licenses_board_{selected_board}.csv\"\n",
    "    results_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"\\nData saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Extract Details for Individual Licenses (Optional)\n",
    "\n",
    "If the search results include links to detailed license pages, we can extract additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_and_save_license_details(results_df, session, max_details=10):\n",
    "    \"\"\"\n",
    "    Extracts details for individual licenses from their detail pages\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pandas DataFrame\n",
    "        DataFrame containing license search results with links\n",
    "    session : requests.Session\n",
    "        Active session to maintain cookies\n",
    "    max_details : int, default=10\n",
    "        Maximum number of licenses to process\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame containing detailed license information\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.myfloridalicense.com/\"\n",
    "    details_list = []\n",
    "    count = 0\n",
    "    \n",
    "    # Check if the DataFrame has a Links column\n",
    "    if 'Links' not in results_df.columns:\n",
    "        print(\"No links found in the results DataFrame.\")\n",
    "        return None\n",
    "    \n",
    "    # Process each link\n",
    "    for index, row in results_df.iterrows():\n",
    "        if count >= max_details:\n",
    "            break\n",
    "        \n",
    "        # Skip rows without links\n",
    "        if pd.isna(row['Links']) or not row['Links']:\n",
    "            continue\n",
    "        \n",
    "        # There might be multiple links separated by '|'\n",
    "        links = row
